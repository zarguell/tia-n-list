name: Simple Daily Digest Generation

on:
  schedule:
    # Run daily at 6:00 AM EST (11:00 AM UTC)
    - cron: '0 11 * * *'
  workflow_dispatch: # Allow manual triggering

jobs:
  generate-simple-digest:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
    - name: Harden the runner (Audit all outbound calls)
      uses: step-security/harden-runner@df199fb7be9f65074067a9eb93f12bb4c5547cf2 # v2.13.3
      with:
        egress-policy: audit

    - name: Checkout repository
      uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        submodules: 'recursive'  # Initialize Hugo theme submodule

    - name: Set up Python 3.13
      uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v6.1.0
      with:
        python-version: '3.13'

    - name: Cache pip dependencies
      uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4.3.0
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create data directory
      run: |
        mkdir -p data

    # - name: Tailscale
    #   uses: tailscale/github-action@a392da0a182bba0e9613b6243ebd69529b1878aa # v4.1.0
    #   with:
    #     oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
    #     oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
    #     tags: tag:github-actions

    # - name: Set exit node
    #   run: |
    #     sudo -E tailscale set --exit-node=${{ secrets.TAILSCALE_EXIT_NODE }} --exit-node-allow-lan-access=true

    - name: Run Simple RSS Feed Ingestion
      env:
        # Core LLM Configuration
        LLM_PROVIDER: ${{ vars.LLM_PROVIDER || 'openrouter' }}
        PYTHONPATH: .

        # OpenAI Configuration (supports custom endpoints)
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
        OPENAI_MODEL: ${{ secrets.OPENAI_MODEL }}
        OPENAI_ANALYSIS_MODEL: ${{ secrets.OPENAI_ANALYSIS_MODEL || secrets.OPENAI_MODEL }}
        OPENAI_FILTERING_MODEL: ${{ secrets.OPENAI_FILTERING_MODEL || secrets.OPENAI_MODEL }}

        # OpenRouter Configuration (fallback)
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        OPENROUTER_MODEL: ${{ secrets.OPENROUTER_MODEL }}
        OPENROUTER_ANALYSIS_MODEL: ${{ vars.OPENROUTER_ANALYSIS_MODEL || 'openai/gpt-oss-20b:free' }}
        OPENROUTER_FILTERING_MODEL: ${{ vars.OPENROUTER_FILTERING_MODEL || 'meta-llama/llama-3.3-8b-instruct:free' }}

        # Gemini Configuration (fallback)
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        GEMINI_FILTERING_MODEL: ${{ secrets.GEMINI_FILTERING_MODEL }}
        GEMINI_ANALYSIS_MODEL: ${{ secrets.GEMINI_ANALYSIS_MODEL }}

        # Storage Configuration
        STORAGE_PROVIDER: ${{ vars.STORAGE_PROVIDER || 'json' }}
      run: |
        echo "ğŸ”„ Starting RSS feed ingestion..."
        echo "Using LLM provider: ${LLM_PROVIDER:-openrouter}"
        echo "ğŸ”§ OpenAI Base URL: ${OPENAI_BASE_URL:-'default'}"
        PYTHONPATH=. python -m src.unified_ingestion
        echo "âœ… RSS ingestion completed"
        echo "ğŸ“Š System statistics:"
        PYTHONPATH=. python -c "from src.storage_registry import get_default_storage_provider; from datetime import date; storage = get_default_storage_provider(); articles = storage.get_articles_by_date_range(start_date=date.today(), end_date=date.today()); print(f'Articles: {len(articles)}')"

    - name: Run Simple LLM Processing (Basic Filtering Only)
      env:
        # Core LLM Configuration
        LLM_PROVIDER: ${{ vars.LLM_PROVIDER || 'openrouter' }}
        PYTHONPATH: .

        # OpenAI Configuration (supports custom endpoints)
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
        OPENAI_MODEL: ${{ secrets.OPENAI_MODEL }}
        OPENAI_ANALYSIS_MODEL: ${{ secrets.OPENAI_ANALYSIS_MODEL || secrets.OPENAI_MODEL }}
        OPENAI_FILTERING_MODEL: ${{ secrets.OPENAI_FILTERING_MODEL || secrets.OPENAI_MODEL }}

        # OpenRouter Configuration (fallback)
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        OPENROUTER_MODEL: ${{ secrets.OPENROUTER_MODEL }}
        OPENROUTER_ANALYSIS_MODEL: ${{ vars.OPENROUTER_ANALYSIS_MODEL || 'openai/gpt-oss-20b:free' }}
        OPENROUTER_FILTERING_MODEL: ${{ vars.OPENROUTER_FILTERING_MODEL || 'meta-llama/llama-3.3-8b-instruct:free' }}

        # Gemini Configuration (fallback)
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        GEMINI_FILTERING_MODEL: ${{ secrets.GEMINI_FILTERING_MODEL }}
        GEMINI_ANALYSIS_MODEL: ${{ secrets.GEMINI_ANALYSIS_MODEL }}

        # Storage Configuration
        STORAGE_PROVIDER: ${{ vars.STORAGE_PROVIDER || 'json' }}
      run: |
        echo "ğŸ§  Starting simple LLM processing (relevance filtering only)..."
        echo "Using LLM provider: ${LLM_PROVIDER:-openrouter}"
        echo "ğŸ”§ OpenAI Base URL: ${OPENAI_BASE_URL:-'default'}"
        echo "ğŸ“Š Processing articles for relevance (no complex IOC extraction)"
        PYTHONPATH=. python scripts/process_simple_articles.py

    - name: Generate Simple Daily Digest
      env:
        # Core LLM Configuration
        LLM_PROVIDER: ${{ vars.LLM_PROVIDER || 'openrouter' }}
        PYTHONPATH: .

        # OpenAI Configuration (supports custom endpoints)
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
        OPENAI_MODEL: ${{ secrets.OPENAI_MODEL }}
        OPENAI_ANALYSIS_MODEL: ${{ secrets.OPENAI_ANALYSIS_MODEL || secrets.OPENAI_MODEL }}
        OPENAI_FILTERING_MODEL: ${{ secrets.OPENAI_FILTERING_MODEL || secrets.OPENAI_MODEL }}
        OPENAI_TITLE_MODEL: ${{ secrets.OPENAI_TITLE_MODEL || secrets.OPENAI_ANALYSIS_MODEL || secrets.OPENAI_MODEL }}

        # OpenRouter Configuration (fallback)
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        OPENROUTER_MODEL: ${{ secrets.OPENROUTER_MODEL }}
        OPENROUTER_ANALYSIS_MODEL: ${{ secrets.OPENROUTER_ANALYSIS_MODEL || 'openai/gpt-oss-20b:free' }}
        OPENROUTER_FILTERING_MODEL: ${{ secrets.OPENROUTER_FILTERING_MODEL || 'meta-llama/llama-3.3-8b-instruct:free' }}
        OPENROUTER_TITLE_MODEL: ${{ secrets.OPENROUTER_TITLE_MODEL || vars.OPENROUTER_ANALYSIS_MODEL || 'openai/gpt-oss-20b:free' }}

        # Gemini Configuration (fallback)
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        GEMINI_FILTERING_MODEL: ${{ secrets.GEMINI_FILTERING_MODEL }}
        GEMINI_ANALYSIS_MODEL: ${{ secrets.GEMINI_ANALYSIS_MODEL }}

        # LLM Token Configuration
        LLM_MAX_TOKENS: ${{ secrets.LLM_MAX_TOKENS || '4000' }}
        LLM_MAX_TOKENS_FILTERING: ${{ secrets.LLM_MAX_TOKENS_FILTERING || '1000' }}
        LLM_MAX_TOKENS_ANALYSIS: ${{ secrets.LLM_MAX_TOKENS_ANALYSIS || '4000' }}
        LLM_MAX_TOKENS_BLOG: ${{ secrets.LLM_MAX_TOKENS_BLOG || '4000' }}
        LLM_TIMEOUT: ${{ secrets.LLM_TIMEOUT || '60' }}

        # Simple Digest Configuration
        USE_SIMPLE_DIGEST: true
        SIMPLE_DIGEST_MAX_ARTICLES: ${{ secrets.SIMPLE_DIGEST_MAX_ARTICLES || '15' }}
        SIMPLE_DIGEST_MAX_CONTENT_LENGTH: ${{ secrets.SIMPLE_DIGEST_MAX_CONTENT_LENGTH || '2000' }}
        SIMPLE_DIGEST_MIN_CONTENT_LENGTH: ${{ secrets.SIMPLE_DIGEST_MIN_CONTENT_LENGTH || '300' }}
        SIMPLE_DIGEST_MAX_REFERENCES: ${{ secrets.SIMPLE_DIGEST_MAX_REFERENCES || '20' }}
        SIMPLE_DIGEST_MEMORY_DAYS_BACK: ${{ secrets.SIMPLE_DIGEST_MEMORY_DAYS_BACK || '14' }}
        SIMPLE_DIGEST_MEMORY_CLEANUP_DAYS: ${{ secrets.SIMPLE_DIGEST_MEMORY_CLEANUP_DAYS || '60' }}
        SIMPLE_DIGEST_MAX_TOKENS: ${{ secrets.SIMPLE_DIGEST_MAX_TOKENS || '4000' }}

        # Storage Configuration
        STORAGE_PROVIDER: ${{ vars.STORAGE_PROVIDER || 'json' }}
      run: |
        echo "ğŸš€ Generating simple daily digest..."
        echo "Using LLM provider: ${LLM_PROVIDER:-openrouter}"
        echo "ğŸ“ Simplified approach: RSS â†’ Basic Processing â†’ Digest Generation"
        echo "ğŸ”§ OpenAI Base URL: ${OPENAI_BASE_URL:-'default'}"
        echo "ğŸ¤– OpenAI Model: ${OPENAI_MODEL:-'default'}"
        PYTHONPATH=. python scripts/generate_simple_digest.py

    - name: Check for generated digest
      id: check-digest
      run: |
        echo "ğŸ” Checking for generated digest..."

        # Check for new digest post
        DIGEST_CHANGES=false
        if [ -n "$(git status --porcelain hugo/content/posts/daily-threat-intelligence-*.md)" ]; then
          DIGEST_CHANGES=true
          echo "âœ… Found new daily digest"
        fi

        # Check JSON data
        JSON_CHANGES=false
        if [ -n "$(git status --porcelain data/)" ]; then
          JSON_CHANGES=true
          echo "âœ… Found JSON data changes"
        fi

        # Set outputs
        echo "digest-changes=$DIGEST_CHANGES" >> $GITHUB_OUTPUT
        echo "json-changes=$JSON_CHANGES" >> $GITHUB_OUTPUT

        if [ "$DIGEST_CHANGES" = "true" ] || [ "$JSON_CHANGES" = "true" ]; then
          echo "has-changes=true" >> $GITHUB_OUTPUT
          echo "ğŸ‰ Found new content to commit"
        else
          echo "has-changes=false" >> $GITHUB_OUTPUT
          echo "â„¹ï¸  No new content generated"
        fi

    - name: Commit and push simple digest
      if: steps.check-digest.outputs.has-changes == 'true'
      run: |
        echo "ğŸ“ Committing simple digest..."

        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"

        # Show what we're committing
        echo "ğŸ“Š Git status:"
        git status

        echo "ğŸ“‹ Changes to be committed:"
        git diff --name-only

        # Stage only source content
        git add data/
        git add hugo/content/posts/daily-threat-intelligence-*.md

        # Verify staged changes
        echo "âœ… Staged changes:"
        git diff --staged --name-only

        # Commit with simple message
        DATE=$(date +'%Y-%m-%d')
        TIME=$(date +'%H:%M:%S')

        git commit -m "ğŸ›¡ï¸ Daily Threat Intelligence Digest - $DATE

        ğŸ“Š Simple Digest Generation:
        â€¢ Digest: ${{ steps.check-digest.outputs.digest-changes }}
        â€¢ JSON Data: ${{ steps.check-digest.outputs.json-changes }}
        â€¢ Generated: $TIME UTC

        ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

        Co-Authored-By: Claude <noreply@anthropic.com>"

        echo "ğŸš€ Pushing changes to repository..."
        git push

    - name: Simple Digest Summary
      run: |
        echo "## ğŸ›¡ï¸ Simple Daily Digest Summary" >> $GITHUB_STEP_SUMMARY
        echo "**Date:** $(date +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ steps.check-digest.outputs.has-changes == 'true' && 'âœ… Digest Generated' || 'â„¹ï¸ No New Content' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### ğŸ“Š Generation Results:" >> $GITHUB_STEP_SUMMARY
        echo "- **Daily Digest:** ${{ steps.check-digest.outputs.digest-changes == 'true' && 'âœ… Generated' || 'â„¹ï¸ No digest' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **JSON Data:** ${{ steps.check-digest.outputs.json-changes == 'true' && 'âœ… Updated' || 'â„¹ï¸ No changes' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Check latest digest using safe file discovery
        # Enable nullglob to handle no matches safely
        shopt -s nullglob
        digest_files=(hugo/content/posts/daily-threat-intelligence-*.md)
        shopt -u nullglob

        if [ ${#digest_files[@]} -gt 0 ]; then
          # Sort files by modification time and get the newest
          LATEST_DIGEST=$(printf '%s\n' "${digest_files[@]}" | sort -t- -k4 -nr | head -1)
          if [ -f "$LATEST_DIGEST" ]; then
            WORD_COUNT=$(wc -w < "$LATEST_DIGEST" 2>/dev/null || echo "0")
            echo "### ğŸ“ Latest Digest:" >> $GITHUB_STEP_SUMMARY
            echo "- **File:** $(basename $LATEST_DIGEST)" >> $GITHUB_STEP_SUMMARY
            echo "- **Word Count:** $WORD_COUNT words" >> $GITHUB_STEP_SUMMARY
          fi
        fi

        # System statistics
        if [ -d "data" ]; then
          ARTICLE_COUNT=$(find data/articles -name "*.json" -type f 2>/dev/null | wc -l)
          PROCESSED_COUNT=$(find data/articles -name "*.json" -type f -exec grep -l '"status": "processed"' {} \; 2>/dev/null | wc -l)
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ“ˆ System Statistics:" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Articles:** $ARTICLE_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Processed Articles:** $PROCESSED_COUNT" >> $GITHUB_STEP_SUMMARY
          if [ "$ARTICLE_COUNT" -gt 0 ]; then
            PROCESSING_RATE=$(awk -v processed="$PROCESSED_COUNT" -v total="$ARTICLE_COUNT" 'BEGIN {printf "%.1f", (processed * 100 / total)}' 2>/dev/null || echo "N/A")
            echo "- **Processing Rate:** ${PROCESSING_RATE}%" >> $GITHUB_STEP_SUMMARY
          fi
        fi

        if [ "${{ steps.check-digest.outputs.has-changes }}" = "true" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸš€ **Next Step:** Hugo build will be triggered automatically" >> $GITHUB_STEP_SUMMARY
        fi