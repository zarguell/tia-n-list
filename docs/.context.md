## Patterns and Conventions

### Python Environment Setup
Always work within the virtual environment and use the proper PYTHONPATH:

```bash
# Create/activate virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies (includes web scraping libraries)
pip install -r requirements.txt

# Run commands with PYTHONPATH set for imports
PYTHONPATH=. pytest tests/ -v
PYTHONPATH=. python -m src.json_ingestion
PYTHONPATH=. python -m src.json_processing
```

**Key Dependencies Added for Content Fetching:**
- `beautifulsoup4>=4.14.0`: HTML parsing and content extraction
- `trafilatura>=2.0.0`: High-quality news article content extraction
- Both libraries include subdependencies for robust web scraping

### Python Style
- All Python code must be formatted with `ruff format`.
- All code must pass `ruff check` and `mypy` static analysis.
- Use type hints for all function signatures and variables where possible.

### Configuration
Configuration is managed in JSON files within the `data/` directory. This keeps settings separate from code and enables git tracking.

```
# Example: data/sources/krebs-on-security.json
{
  "id": "krebs-on-security",
  "name": "Krebs on Security",
  "url": "https://krebsonsecurity.com/feed/",
  "active": true,
  "metadata": {
    "focus_areas": ["cybercrime", "fraud", "data-breaches"]
  }
}
```

### JSON Storage Abstraction
Do not write raw file operations in application logic. Use the functions provided in `src/json_storage.py` for all data interactions.

```
# Example: Using the JSON storage module
from src.json_storage import JSONStorage

storage = JSONStorage()

# Add articles
article_id = storage.add_article(
    source_id="krebs-on-security",
    title="Article Title",
    url="https://example.com/article",
    published_at=datetime.now(timezone.utc),
    raw_content="Article content..."
)

# Get articles
articles = storage.get_recent_articles(days=7, status="processed")
```

### Multi-Provider LLM Client Configuration
The system now supports multiple LLM providers (Gemini, OpenRouter, OpenAI) with automatic fallback:

```python
# Multi-provider client with automatic fallback
from src.llm_client_multi import MultiLLMClient
client = MultiLLMClient()

# Check provider configuration
info = client.get_provider_info()
print(f"Primary provider: {info['primary_provider']}")
print(f"Fallback provider: {info['fallback_provider']}")
```

**Provider Selection via Environment Variables:**
- `LLM_PROVIDER=gemini` (default) - Uses Google Gemini API
- `LLM_PROVIDER=openrouter` - Uses OpenRouter with free models
- `LLM_PROVIDER=openai` - Uses OpenAI API

**Configuration Examples:**

```bash
# Use OpenRouter with free models
export LLM_PROVIDER=openrouter
export OPENROUTER_API_KEY=your_key_here
export OPENROUTER_FILTERING_MODEL=meta-llama/llama-3.3-8b-instruct:free
export OPENROUTER_ANALYSIS_MODEL=openai/gpt-oss-20b:free

# Use OpenAI
export LLM_PROVIDER=openai
export OPENAI_API_KEY=your_key_here

# Fallback configuration - tries OpenRouter first, falls back to Gemini
export LLM_PROVIDER=openrouter
export OPENROUTER_API_KEY=your_key_here
export GEMINI_API_KEY=your_gemini_key_here
```

**Free Model Priority:**
- OpenRouter: `meta-llama/llama-3.3-8b-instruct:free` (filtering), `openai/gpt-oss-20b:free` (analysis)
- Gemini: `gemini-2.0-flash-lite` (filtering), `gemini-2.5-flash` (analysis)

### Content Fetching System

The system includes a modular web scraping architecture to enhance RSS feed content when feeds only provide partial content.

**Core Components:**
- `src/content_fetcher.py`: Main content fetching module with rate limiting and respectful scraping
- `src/extractors/`: Modular website-specific extractor system
  - `base.py`: Abstract base class for all extractors
  - `threatpost.py`: Threatpost-specific content extraction
  - `schneier.py`: Schneier on Security content extraction
  - `__init__.py`: Extractor registry and auto-discovery system

**Usage Examples:**
```python
# Simple content fetching
from src.content_fetcher import fetch_article_content
result = fetch_article_content('https://example.com/article')

# Extractor registry management
from src.extractors import get_extractor_registry, get_extractor_for_url
registry = get_extractor_registry()
extractor = get_extractor_for_url('https://threatpost.com/article')

# Adding new extractors
from src.extractors import register_extractor, BaseExtractor
class MySiteExtractor(BaseExtractor):
    def get_supported_domains(self):
        return ['mysite.com']
    def extract_content(self, url, html_content):
        # Site-specific extraction logic
        return cleaned_content

register_extractor('mysite', MySiteExtractor)
```

**Extraction Priority Chain:**
1. **Website-specific extractor** (highest quality, site-optimized)
2. **Trafilatura** (excellent for news articles, general purpose)
3. **BeautifulSoup fallback** (basic content extraction)

**Performance Results:**
- Threatpost articles: 108 chars ‚Üí 4,139 chars (3,732% improvement)
- Schneier articles: 1,456 chars ‚Üí 1,552 chars (cleaner, better structured)
- Automatic rate limiting (1-3 second delays) and respectful headers included

### Dynamic Content Generation System

The system includes intelligent title and tag generation for engaging blog posts:

**Title Generation System:**
```python
# Dynamic title generation
from src.title_generator import TitleGenerator
from src.json_storage import JSONStorage

storage = JSONStorage()
title_generator = TitleGenerator(storage)

# Generate title for today's articles
from datetime import date
articles = storage.get_articles_by_date_range(
    start_date=date.today(),
    end_date=date.today(),
    status='processed'
)

title = title_generator.generate_title(articles, date.today())
# Output: "üè¢ Microsoft Security Issues Widespread in Latest Intelligence - October 29, 2025"
```

**Tag Generation System:**
```python
# Intelligent tag extraction
from src.tag_generator import TagGenerator

tag_generator = TagGenerator(storage)

# Generate tags for today's articles
tags = tag_generator.generate_tags_for_date(date.today(), limit=15)

# Format for Hugo frontmatter
hugo_tags = tag_generator.format_tags_for_hugo(tags)
# Output: ["cvss", "remote-code-execution", "microsoft", "critical", ...]
```

**Enhanced Blog Generation:**
```python
# Complete enhanced blog generation
from src.enhanced_json_blog_generator import EnhancedJSONBlogGenerator

enhanced_generator = EnhancedJSONBlogGenerator()
result = enhanced_generator.generate_daily_summary(
    target_date=date.today(),
    use_intelligent_synthesis=True,
    use_dynamic_title=True,
    use_dynamic_tags=True
)
```

### Development Commands
```bash
# Testing and code quality (with venv activated and PYTHONPATH set)
pytest tests/ -v --cov=src
ruff check src/ tests/
mypy src/

# JSON-based core operations
PYTHONPATH=. python -m src.json_ingestion  # Run feed ingestion with JSON storage
PYTHONPATH=. python -m src.json_processing  # Run LLM processing with JSON storage
PYTHONPATH=. python -m src.json_blog_generator  # Generate blog posts with JSON context

# Enhanced blog generation with dynamic content
PYTHONPATH=. python scripts/generate_enhanced_blog.py  # Enhanced blog generation
PYTHONPATH=. python src/enhanced_json_blog_generator   # Direct enhanced generation
PYTHONPATH=. python src.title_generator  # Test title generation
PYTHONPATH=. python src.tag_generator    # Test tag generation

# Dynamic content testing
PYTHONPATH=. python scripts/test_title_generator.py  # Test title generation
PYTHONPATH=. python scripts/test_tag_generator.py    # Test tag generation
PYTHONPATH=. python scripts/test_integrated_generation.py  # Test integration

# Context and storage testing
PYTHONPATH=. python -c "from src.json_storage import JSONStorage; storage = JSONStorage(); print('JSON storage ready')"
PYTHONPATH=. python -c "from src.context_builder import AIContextBuilder; builder = AIContextBuilder(); print('Context builder ready')"

# System statistics and analysis
PYTHONPATH=. python -c "from src.json_storage import JSONStorage; print(JSONStorage().get_statistics())"

# Content fetching and enhancement (when dependencies available)
PYTHONPATH=. python -c "from src.content_fetcher import fetch_article_content; print('Content fetcher ready')"
PYTHONPATH=. python -c "from src.extractors import get_extractor_registry; print('Extractor registry ready')"

# LLM client testing
PYTHONPATH=. python -c "from src.llm_client_multi import MultiLLMClient; client = MultiLLMClient(); print('Multi-provider LLM client ready')"

# Hugo site generation
hugo -s hugo/ --minify  # Build static site
```

### JSON-Based Architecture Benefits

**‚úÖ GitHub Actions Ready:**
- All content persists in git between runs
- No database setup required in CI/CD
- Content history tracked in git
- Easy rollback and debugging

**‚úÖ Git-Friendly Features:**
- Human-readable JSON files
- Diff-able content changes
- Branch-based testing possible
- Content review via pull requests

**‚úÖ AI-Optimized:**
- Intelligent context building
- Content similarity detection
- Threat landscape analysis
- Flexible prompting system

### JSON-Based Processing Workflow

1. **RSS Ingestion**: `src/json_ingestion.py` fetches feeds and stores in `data/articles/`
2. **Content Enhancement**: Full content fetched and stored in `data/content/`
3. **AI Processing**: `src/json_processing.py` analyzes content and extracts IOCs
4. **Context Building**: `src/context_builder.py` creates AI context from processed data
5. **Blog Generation**: `src/json_blog_generator.py` generates Hugo posts
6. **Memory Management**: `data/report_memory.json` tracks reported articles for deduplication
7. **Git Persistence**: All content committed to git for history and CI/CD

### Article Deduplication System

The system includes a 7-day rolling memory to prevent duplicate articles across consecutive daily briefings:

**Memory File Structure (`data/report_memory.json`)**:
```json
{
  "reports": {
    "2025-10-30T08:28:01.994520": {
      "article_ids": ["article-id-1", "article-id-2"],
      "content_length": 10954,
      "mentioned_cves": ["CVE-2025-1234"],
      "mentioned_cisa_ids": ["AA-25-123A"]
    }
  },
  "mentioned_articles": ["article-id-1", "article-id-2"],
  "mentioned_cves": ["CVE-2025-1234"],
  "mentioned_cisa_ids": ["AA-25-123A"]
}
```

**Deduplication Process**:
1. **Memory Loading**: Convert JSON lists to sets for efficient operations
2. **Date Filtering**: Process reports from last 7 days using ISO datetime parsing
3. **Article Filtering**: Compare current article IDs against memory to identify duplicates
4. **Fresh Content**: Only process articles not recently reported
5. **Memory Update**: Save new articles to memory after successful synthesis

**Key Implementation Details**:
- **Timezone Handling**: Proper UTC datetime comparisons with ISO format support
- **Robust Parsing**: Handles both `'YYYY-MM-DD'` and `'YYYY-MM-DDTHH:MM:SS.ssssss'` formats
- **Set Operations**: Efficient duplicate detection using set intersections
- **Error Prevention**: Safe dictionary access prevents crashes from malformed data

### Scalable Processing Architecture

The system now implements intelligent feed processing to handle large volumes efficiently:

**5-Tier Processing System:**
- **Priority 1**: High-quality sources (70+ score) ‚Üí Immediate full processing
- **Priority 2**: Medium-quality sources with relevant keywords ‚Üí Full processing
- **Priority 3**: Low-quality sources with relevant titles ‚Üí Selective processing
- **Priority 4**: Low relevance titles ‚Üí Title analysis only
- **Priority 5**: Very low relevance ‚Üí Skipped/deprioritized

**Source Quality Scoring (100-point scale):**
- Content Length (40 points): Average article length √∑ 2000 chars
- Content Completeness (30 points): % of articles with >1000 chars
- Relevance Rate (20 points): % of articles accepted in LLM processing
- Consistency (10 points): Low variation in content length

**Processing Efficiency:**
- **85.7% reduction** in LLM processing (14.3% of articles processed)
- **$0.42 savings** per processing cycle
- **60% acceptance rate** for analyzed articles
- **24-hour time filtering** prevents data overload
- **50 article limit** per feed enforces quantity control

**OpenRouter Fallback Models:**
```python
fallback_models = [
    'z-ai/glm-4.5-air:free',        # Primary fallback
    'qwen/qwen3-235b-a22b:free',     # 1st fallback
    'microsoft/mai-ds-r1:free',      # 2nd fallback
    'google/gemini-2.0-flash-exp:free' # 3rd fallback
]
```

### GitHub Actions Workflow
Workflows should be modular. The daily job will have distinct steps for:
1.  Setup Python & Dependencies
2.  Run Ingestion (with time filtering and article limits)
3.  Run Scalable Processing (tiered analysis)
4.  Run Content Generation
5.  Build and Deploy Site
6.  Send Newsletter
