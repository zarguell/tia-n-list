## Patterns and Conventions

### Python Environment Setup
Always work within the virtual environment and use the proper PYTHONPATH:

```bash
# Create/activate virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies (includes web scraping libraries)
pip install -r requirements.txt

# Run commands with PYTHONPATH set for imports
PYTHONPATH=. pytest tests/ -v
PYTHONPATH=. python -m src.ingestion
```

**Key Dependencies Added for Content Fetching:**
- `beautifulsoup4>=4.14.0`: HTML parsing and content extraction
- `trafilatura>=2.0.0`: High-quality news article content extraction
- Both libraries include subdependencies for robust web scraping

### Python Style
- All Python code must be formatted with `ruff format`.
- All code must pass `ruff check` and `mypy` static analysis.
- Use type hints for all function signatures and variables where possible.

### Configuration
Configuration is managed in YAML files within the `config/` directory. This keeps settings separate from code.

```
# Example: config/feeds.yml
feeds:
  - name: "Krebs on Security"
    url: "https://krebsonsecurity.com/feed/"
  - name: "Schneier on Security"
    url: "https://www.schneier.com/feed/atom/"
```

### Database Abstraction
Do not write raw SQL queries in application logic. Use the functions provided in `src/database.py` for all database interactions.

```
# Example: Using the database module
from src import database

# All connections are handled within the functions
new_articles = [...] # List of article dicts
database.add_articles(new_articles)

processed_article = database.get_article_by_id(1)
```

### Multi-Provider LLM Client Configuration
The system now supports multiple LLM providers (Gemini, OpenRouter, OpenAI) with automatic fallback:

```python
# Multi-provider client with automatic fallback
from src.llm_client_multi import MultiLLMClient
client = MultiLLMClient()

# Check provider configuration
info = client.get_provider_info()
print(f"Primary provider: {info['primary_provider']}")
print(f"Fallback provider: {info['fallback_provider']}")
```

**Provider Selection via Environment Variables:**
- `LLM_PROVIDER=gemini` (default) - Uses Google Gemini API
- `LLM_PROVIDER=openrouter` - Uses OpenRouter with free models
- `LLM_PROVIDER=openai` - Uses OpenAI API

**Configuration Examples:**

```bash
# Use OpenRouter with free models
export LLM_PROVIDER=openrouter
export OPENROUTER_API_KEY=your_key_here
export OPENROUTER_FILTERING_MODEL=meta-llama/llama-3.3-8b-instruct:free
export OPENROUTER_ANALYSIS_MODEL=openai/gpt-oss-20b:free

# Use OpenAI
export LLM_PROVIDER=openai
export OPENAI_API_KEY=your_key_here

# Fallback configuration - tries OpenRouter first, falls back to Gemini
export LLM_PROVIDER=openrouter
export OPENROUTER_API_KEY=your_key_here
export GEMINI_API_KEY=your_gemini_key_here
```

**Free Model Priority:**
- OpenRouter: `meta-llama/llama-3.3-8b-instruct:free` (filtering), `openai/gpt-oss-20b:free` (analysis)
- Gemini: `gemini-2.0-flash-lite` (filtering), `gemini-2.5-flash` (analysis)

### Content Fetching System

The system includes a modular web scraping architecture to enhance RSS feed content when feeds only provide partial content.

**Core Components:**
- `src/content_fetcher.py`: Main content fetching module with rate limiting and respectful scraping
- `src/extractors/`: Modular website-specific extractor system
  - `base.py`: Abstract base class for all extractors
  - `threatpost.py`: Threatpost-specific content extraction
  - `schneier.py`: Schneier on Security content extraction
  - `__init__.py`: Extractor registry and auto-discovery system

**Usage Examples:**
```python
# Simple content fetching
from src.content_fetcher import fetch_article_content
result = fetch_article_content('https://example.com/article')

# Extractor registry management
from src.extractors import get_extractor_registry, get_extractor_for_url
registry = get_extractor_registry()
extractor = get_extractor_for_url('https://threatpost.com/article')

# Adding new extractors
from src.extractors import register_extractor, BaseExtractor
class MySiteExtractor(BaseExtractor):
    def get_supported_domains(self):
        return ['mysite.com']
    def extract_content(self, url, html_content):
        # Site-specific extraction logic
        return cleaned_content

register_extractor('mysite', MySiteExtractor)
```

**Extraction Priority Chain:**
1. **Website-specific extractor** (highest quality, site-optimized)
2. **Trafilatura** (excellent for news articles, general purpose)
3. **BeautifulSoup fallback** (basic content extraction)

**Performance Results:**
- Threatpost articles: 108 chars → 4,139 chars (3,732% improvement)
- Schneier articles: 1,456 chars → 1,552 chars (cleaner, better structured)
- Automatic rate limiting (1-3 second delays) and respectful headers included

### Development Commands
```bash
# Testing and code quality (with venv activated and PYTHONPATH set)
pytest tests/ -v --cov=src
ruff check src/ tests/
mypy src/

# Core operations
python -m src.ingestion  # Run feed ingestion (with 24-hour filtering and 50 article limit)
python -m src.processing  # Run LLM processing
python -m src.blog_generator  # Generate blog posts (original)
python -m src.enhanced_blog_generator  # Generate enhanced blog posts with categorization
python -m src.intelligent_blog_generator  # Generate intelligent LLM-synthesized blog posts
python -m src.newsletter_generator  # Generate newsletter
python -m src.distribution  # Send newsletter

# Scalable processing and analysis
python -m src.source_quality  # Analyze and track source quality over time
python -m src.tiered_processor  # Run multi-tier article processing (14.3% efficiency)
python -m src.scalable_processor  # Run optimized LLM processing for large feed volumes
python scripts/analyze_feed_quality.py  # Analyze RSS feed content quality without parsers

# Content fetching and testing
python -c "from src.content_fetcher import fetch_article_content; print('Content fetcher ready')"
python -c "from src.extractors import get_extractor_registry; print('Extractor registry ready')"
python -c "from src.extractors import get_extractor_for_url; print('URL-based extractor selection ready')"

# LLM client testing
python -c "from src.llm_client import LLMClient; client = LLMClient(); print('LLM client ready')"
python -c "from src.llm_client_multi import MultiLLMClient; client = MultiLLMClient(); print('Multi-provider LLM client ready')"

# Intelligent synthesis testing
python -c "from src.intelligent_blog_generator import ThreatIntelligenceSynthesizer; synthesizer = ThreatIntelligenceSynthesizer(); print('Intelligent synthesis ready')"
```

### Scalable Processing Architecture

The system now implements intelligent feed processing to handle large volumes efficiently:

**5-Tier Processing System:**
- **Priority 1**: High-quality sources (70+ score) → Immediate full processing
- **Priority 2**: Medium-quality sources with relevant keywords → Full processing
- **Priority 3**: Low-quality sources with relevant titles → Selective processing
- **Priority 4**: Low relevance titles → Title analysis only
- **Priority 5**: Very low relevance → Skipped/deprioritized

**Source Quality Scoring (100-point scale):**
- Content Length (40 points): Average article length ÷ 2000 chars
- Content Completeness (30 points): % of articles with >1000 chars
- Relevance Rate (20 points): % of articles accepted in LLM processing
- Consistency (10 points): Low variation in content length

**Processing Efficiency:**
- **85.7% reduction** in LLM processing (14.3% of articles processed)
- **$0.42 savings** per processing cycle
- **60% acceptance rate** for analyzed articles
- **24-hour time filtering** prevents data overload
- **50 article limit** per feed enforces quantity control

**OpenRouter Fallback Models:**
```python
fallback_models = [
    'z-ai/glm-4.5-air:free',        # Primary fallback
    'qwen/qwen3-235b-a22b:free',     # 1st fallback
    'microsoft/mai-ds-r1:free',      # 2nd fallback
    'google/gemini-2.0-flash-exp:free' # 3rd fallback
]
```

### GitHub Actions Workflow
Workflows should be modular. The daily job will have distinct steps for:
1.  Setup Python & Dependencies
2.  Run Ingestion (with time filtering and article limits)
3.  Run Scalable Processing (tiered analysis)
4.  Run Content Generation
5.  Build and Deploy Site
6.  Send Newsletter
