## Patterns and Conventions

### Python Environment Setup
Always work within the virtual environment and use the proper PYTHONPATH:

```bash
# Create/activate virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies (includes web scraping libraries)
pip install -r requirements.txt

# Run commands with PYTHONPATH set for imports
PYTHONPATH=. pytest tests/ -v
PYTHONPATH=. python -m src.json_ingestion
PYTHONPATH=. python -m src.json_processing
```

**Key Dependencies Added for Content Fetching:**
- `beautifulsoup4>=4.14.0`: HTML parsing and content extraction
- `trafilatura>=2.0.0`: High-quality news article content extraction
- Both libraries include subdependencies for robust web scraping

### Python Style
- All Python code must be formatted with `ruff format`.
- All code must pass `ruff check` and `mypy` static analysis.
- Use type hints for all function signatures and variables where possible.

### Configuration
Configuration is managed in JSON files within both the `data/` and `config/` directories. This keeps settings separate from code and enables git tracking.

```
# Example: data/sources/krebs-on-security.json
{
  "id": "krebs-on-security",
  "name": "Krebs on Security",
  "url": "https://krebsonsecurity.com/feed/",
  "active": true,
  "metadata": {
    "focus_areas": ["cybercrime", "fraud", "data-breaches"]
  }
}
```

### Prompt Configuration System
The system includes a configurable prompt architecture for dynamic prompt management:

**Prompt Templates:** JSON-based templates in `config/prompts/` directory
- `threat_intelligence_synthesis.json`: Main synthesis prompt with sections and variables
- `confidence_assessment.json`: Industry-standard confidence level guidance
- `mitre_attack_guidance.json`: MITRE ATT&CK technique integration
- `industry_impact_guidance.json`: Industry-specific impact analysis
- `tlp_banner.json`: TLP marking templates and AI disclaimers

**Usage Examples:**
```python
# Load and use prompt configuration
from src.prompt_loader import PromptLoader

loader = PromptLoader(Path("config/prompts"))
template = loader.load_main_prompt()

# Build enhanced synthesis prompt
prompt = loader.build_enhanced_synthesis_prompt(
    articles_data=articles_string,
    factual_constraints=constraints_string,
    current_date="2025-10-30"
)
```

**Prompt Versioning and A/B Testing:**
```python
# Version management and A/B testing
from src.prompt_loader import PromptVersionManager

manager = PromptVersionManager(Path("config/prompts"))

# Load specific version
template = manager.load_versioned_prompt("synthesis", "1.2.0")

# List available versions
versions = manager.list_available_versions("synthesis")
# Returns: ["1.0.0", "1.1.0", "1.2.0", "2.0.0-beta"]

# Get latest version
latest = manager.get_latest_version("synthesis")

# A/B testing variant selection
variant = manager.select_ab_test_variant(template)
# Returns: "control" or "experimental" based on weights

# Create backup before modifications
backup_path = manager.create_version_backup("synthesis", config_dict)
```

**Versioning Features:**
- Semantic version parsing and comparison (1.2.0 vs 1.1.0)
- Versioned prompt files (synthesis_v1.2.0.json)
- Automatic backup system with timestamps
- Upgrade path calculation between versions
- Weighted A/B testing with configurable variants
- Changelog tracking and metadata support

**Enhanced Prompt Configuration System (v2.0+):**
The prompt system now includes comprehensive enterprise-grade CTI enhancements:

**Executive-Grade Prompt Templates:**
- `threat_intelligence_synthesis.json` (v2.0.0): Enhanced for C-level executive audience
- `confidence_assessment.json`: Industry-standard confidence level guidance with executive framing
- `mitre_attack_guidance.json`: Comprehensive MITRE ATT&CK integration with detection implications
- `industry_impact_guidance.json`: Business impact analysis with sector-specific exposure assessment
- `intelligence_gap_guidance.json`: Intelligence gap analysis for transparency and collection priorities
- `tlp_banner.json`: TLP marking templates and AI disclaimers

**Usage Examples:**
```python
# Load enhanced synthesis prompt with all guidance
from src.prompt_loader import PromptLoader

loader = PromptLoader(Path("config/prompts"))

# Build comprehensive synthesis prompt
prompt = loader.build_enhanced_synthesis_prompt(
    articles_data=articles_string,
    factual_constraints=constraints_string,
    current_date="2025-10-30"
)
# Includes: confidence assessments, MITRE ATT&CK, industry impact, intelligence gaps

# Load specific guidance components
confidence_guidance = loader.load_confidence_guidance()
attack_guidance = loader.load_mitre_attack_guidance()
gap_guidance = loader.load_intelligence_gap_guidance()
```

**Enhanced Features:**
- **Confidence Assessment Framework**: High/Medium/Low confidence with executive decision-making context
- **MITRE ATT&CK Integration**: Technique references with detection guidance and response priorities
- **Industry Impact Analysis**: Business-focused impact assessment with sector-specific exposure
- **Intelligence Gap Identification**: Transparency about limitations and collection priorities
- **Executive Tone Optimization**: C-level briefing format with business impact prioritization
- **A/B Testing Framework**: Weighted variant testing for prompt optimization

**Benefits:**
- Template-based prompts without code changes
- Variable substitution for dynamic content
- Safe experimental testing with A/B framework
- Version control with rollback capabilities
- Industry-standard confidence and ATT&CK integration
- Enterprise-grade intelligence gap analysis
- C-level executive focus and business impact orientation
- Systematic prompt optimization and testing

### Professional Persona Module
The persona module provides professional executive-focused content formatting:

**Updated Persona Functions:**
- `format_article_summary()`: Professional article formatting with relevance indicators and executive insights
- `get_top_articles_for_summary()`: Retrieves high-priority articles for executive briefings

**Professional Tone Guidelines:**
- Content focused on C-level executive audience
- Relevance scoring with actionable insights (🔴 Low, 🟡 Medium, 🟢 High priority)
- Executive-appropriate analysis without casual humor
- Professional risk assessment language

**Usage Examples:**
```python
# Professional article formatting
from src.persona import format_article_summary

summary = format_article_summary(
    title="Critical Vulnerability in Enterprise Software",
    content="Remote code execution vulnerability affects...",
    score=85
)
# Returns professional summary with executive insights
```

**Removed for Professional Standards:**
- Joke API integration (removed for executive audience appropriateness)
- Casual content elements that undermine professional credibility
- Non-essential humor functions

### JSON Storage Abstraction
Do not write raw file operations in application logic. Use the functions provided in `src/json_storage.py` for all data interactions.

```
# Example: Using the JSON storage module
from src.json_storage import JSONStorage

storage = JSONStorage()

# Add articles
article_id = storage.add_article(
    source_id="krebs-on-security",
    title="Article Title",
    url="https://example.com/article",
    published_at=datetime.now(timezone.utc),
    raw_content="Article content..."
)

# Get articles
articles = storage.get_recent_articles(days=7, status="processed")
```

### Multi-Provider LLM Client Configuration
The system now supports multiple LLM providers (Gemini, OpenRouter, OpenAI) with automatic fallback:

```python
# Multi-provider client with automatic fallback
from src.llm_client_multi import MultiLLMClient
client = MultiLLMClient()

# Check provider configuration
info = client.get_provider_info()
print(f"Primary provider: {info['primary_provider']}")
print(f"Fallback provider: {info['fallback_provider']}")
```

**Provider Selection via Environment Variables:**
- `LLM_PROVIDER=gemini` (default) - Uses Google Gemini API
- `LLM_PROVIDER=openrouter` - Uses OpenRouter with free models
- `LLM_PROVIDER=openai` - Uses OpenAI API

**Configuration Examples:**

```bash
# Use OpenRouter with free models
export LLM_PROVIDER=openrouter
export OPENROUTER_API_KEY=your_key_here
export OPENROUTER_FILTERING_MODEL=meta-llama/llama-3.3-8b-instruct:free
export OPENROUTER_ANALYSIS_MODEL=openai/gpt-oss-20b:free

# Use OpenAI
export LLM_PROVIDER=openai
export OPENAI_API_KEY=your_key_here

# Fallback configuration - tries OpenRouter first, falls back to Gemini
export LLM_PROVIDER=openrouter
export OPENROUTER_API_KEY=your_key_here
export GEMINI_API_KEY=your_gemini_key_here
```

**Free Model Priority:**
- OpenRouter: `meta-llama/llama-3.3-8b-instruct:free` (filtering), `openai/gpt-oss-20b:free` (analysis)
- Gemini: `gemini-2.0-flash-lite` (filtering), `gemini-2.5-flash` (analysis)

### Content Fetching System

The system includes a modular web scraping architecture to enhance RSS feed content when feeds only provide partial content.

**Core Components:**
- `src/content_fetcher.py`: Main content fetching module with rate limiting and respectful scraping
- `src/extractors/`: Modular website-specific extractor system
  - `base.py`: Abstract base class for all extractors
  - `threatpost.py`: Threatpost-specific content extraction
  - `schneier.py`: Schneier on Security content extraction
  - `__init__.py`: Extractor registry and auto-discovery system

**Usage Examples:**
```python
# Simple content fetching
from src.content_fetcher import fetch_article_content
result = fetch_article_content('https://example.com/article')

# Extractor registry management
from src.extractors import get_extractor_registry, get_extractor_for_url
registry = get_extractor_registry()
extractor = get_extractor_for_url('https://threatpost.com/article')

# Adding new extractors
from src.extractors import register_extractor, BaseExtractor
class MySiteExtractor(BaseExtractor):
    def get_supported_domains(self):
        return ['mysite.com']
    def extract_content(self, url, html_content):
        # Site-specific extraction logic
        return cleaned_content

register_extractor('mysite', MySiteExtractor)
```

**Extraction Priority Chain:**
1. **Website-specific extractor** (highest quality, site-optimized)
2. **Trafilatura** (excellent for news articles, general purpose)
3. **BeautifulSoup fallback** (basic content extraction)

**Performance Results:**
- Threatpost articles: 108 chars → 4,139 chars (3,732% improvement)
- Schneier articles: 1,456 chars → 1,552 chars (cleaner, better structured)
- Automatic rate limiting (1-3 second delays) and respectful headers included

### Dynamic Content Generation System

The system includes intelligent title and tag generation for engaging blog posts:

**Title Generation System:**
```python
# Dynamic title generation
from src.title_generator import TitleGenerator
from src.json_storage import JSONStorage

storage = JSONStorage()
title_generator = TitleGenerator(storage)

# Generate title for today's articles
from datetime import date
articles = storage.get_articles_by_date_range(
    start_date=date.today(),
    end_date=date.today(),
    status='processed'
)

title = title_generator.generate_title(articles, date.today())
# Output: "🏢 Microsoft Security Issues Widespread in Latest Intelligence - October 29, 2025"
```

**Tag Generation System:**
```python
# Intelligent tag extraction
from src.tag_generator import TagGenerator

tag_generator = TagGenerator(storage)

# Generate tags for today's articles
tags = tag_generator.generate_tags_for_date(date.today(), limit=15)

# Format for Hugo frontmatter
hugo_tags = tag_generator.format_tags_for_hugo(tags)
# Output: ["cvss", "remote-code-execution", "microsoft", "critical", ...]
```

**Enhanced Blog Generation:**
```python
# Complete enhanced blog generation
from src.enhanced_json_blog_generator import EnhancedJSONBlogGenerator

enhanced_generator = EnhancedJSONBlogGenerator()
result = enhanced_generator.generate_daily_summary(
    target_date=date.today(),
    use_intelligent_synthesis=True,
    use_dynamic_title=True,
    use_dynamic_tags=True
)
```

### Development Commands
```bash
# Testing and code quality (with venv activated and PYTHONPATH set)
pytest tests/ -v --cov=src
ruff check src/ tests/
mypy src/

# JSON-based core operations
PYTHONPATH=. python -m src.json_ingestion  # Run feed ingestion with JSON storage
PYTHONPATH=. python -m src.json_processing  # Run LLM processing with JSON storage
PYTHONPATH=. python -m src.json_blog_generator  # Generate blog posts with JSON context

# Enhanced blog generation with dynamic content
PYTHONPATH=. python scripts/generate_enhanced_blog.py  # Enhanced blog generation
PYTHONPATH=. python src/enhanced_json_blog_generator   # Direct enhanced generation
PYTHONPATH=. python src.title_generator  # Test title generation
PYTHONPATH=. python src.tag_generator    # Test tag generation

# Dynamic content testing
PYTHONPATH=. python scripts/test_title_generator.py  # Test title generation
PYTHONPATH=. python scripts/test_tag_generator.py    # Test tag generation
PYTHONPATH=. python scripts/test_integrated_generation.py  # Test integration

# Context and storage testing
PYTHONPATH=. python -c "from src.json_storage import JSONStorage; storage = JSONStorage(); print('JSON storage ready')"
PYTHONPATH=. python -c "from src.context_builder import AIContextBuilder; builder = AIContextBuilder(); print('Context builder ready')"

# System statistics and analysis
PYTHONPATH=. python -c "from src.json_storage import JSONStorage; print(JSONStorage().get_statistics())"

# Content fetching and enhancement (when dependencies available)
PYTHONPATH=. python -c "from src.content_fetcher import fetch_article_content; print('Content fetcher ready')"
PYTHONPATH=. python -c "from src.extractors import get_extractor_registry; print('Extractor registry ready')"

# LLM client testing
PYTHONPATH=. python -c "from src.llm_client_multi import MultiLLMClient; client = MultiLLMClient(); print('Multi-provider LLM client ready')"

# Hugo site generation
hugo -s hugo/ --minify  # Build static site
```

### JSON-Based Architecture Benefits

**✅ GitHub Actions Ready:**
- All content persists in git between runs
- No database setup required in CI/CD
- Content history tracked in git
- Easy rollback and debugging

**✅ Git-Friendly Features:**
- Human-readable JSON files
- Diff-able content changes
- Branch-based testing possible
- Content review via pull requests

**✅ AI-Optimized:**
- Intelligent context building
- Content similarity detection
- Threat landscape analysis
- Flexible prompting system

### JSON-Based Processing Workflow

1. **RSS Ingestion**: `src/json_ingestion.py` fetches feeds and stores in `data/articles/`
2. **Content Enhancement**: Full content fetched and stored in `data/content/`
3. **AI Processing**: `src/json_processing.py` analyzes content and extracts IOCs
4. **Context Building**: `src/context_builder.py` creates AI context from processed data
5. **Blog Generation**: `src/json_blog_generator.py` generates Hugo posts
6. **Memory Management**: `data/report_memory.json` tracks reported articles for deduplication
7. **Git Persistence**: All content committed to git for history and CI/CD

### Article Deduplication System

The system includes a 7-day rolling memory to prevent duplicate articles across consecutive daily briefings:

**Memory File Structure (`data/report_memory.json`)**:
```json
{
  "reports": {
    "2025-10-30T08:28:01.994520": {
      "article_ids": ["article-id-1", "article-id-2"],
      "content_length": 10954,
      "mentioned_cves": ["CVE-2025-1234"],
      "mentioned_cisa_ids": ["AA-25-123A"]
    }
  },
  "mentioned_articles": ["article-id-1", "article-id-2"],
  "mentioned_cves": ["CVE-2025-1234"],
  "mentioned_cisa_ids": ["AA-25-123A"]
}
```

**Deduplication Process**:
1. **Memory Loading**: Convert JSON lists to sets for efficient operations
2. **Date Filtering**: Process reports from last 7 days using ISO datetime parsing
3. **Article Filtering**: Compare current article IDs against memory to identify duplicates
4. **Fresh Content**: Only process articles not recently reported
5. **Memory Update**: Save new articles to memory after successful synthesis

**Key Implementation Details**:
- **Timezone Handling**: Proper UTC datetime comparisons with ISO format support
- **Robust Parsing**: Handles both `'YYYY-MM-DD'` and `'YYYY-MM-DDTHH:MM:SS.ssssss'` formats
- **Set Operations**: Efficient duplicate detection using set intersections
- **Error Prevention**: Safe dictionary access prevents crashes from malformed data

### Scalable Processing Architecture

The system now implements intelligent feed processing to handle large volumes efficiently:

**5-Tier Processing System:**
- **Priority 1**: High-quality sources (70+ score) → Immediate full processing
- **Priority 2**: Medium-quality sources with relevant keywords → Full processing
- **Priority 3**: Low-quality sources with relevant titles → Selective processing
- **Priority 4**: Low relevance titles → Title analysis only
- **Priority 5**: Very low relevance → Skipped/deprioritized

**Source Quality Scoring (100-point scale):**
- Content Length (40 points): Average article length ÷ 2000 chars
- Content Completeness (30 points): % of articles with >1000 chars
- Relevance Rate (20 points): % of articles accepted in LLM processing
- Consistency (10 points): Low variation in content length

**Processing Efficiency:**
- **85.7% reduction** in LLM processing (14.3% of articles processed)
- **$0.42 savings** per processing cycle
- **60% acceptance rate** for analyzed articles
- **24-hour time filtering** prevents data overload
- **50 article limit** per feed enforces quantity control

**OpenRouter Fallback Models:**
```python
fallback_models = [
    'z-ai/glm-4.5-air:free',        # Primary fallback
    'qwen/qwen3-235b-a22b:free',     # 1st fallback
    'microsoft/mai-ds-r1:free',      # 2nd fallback
    'google/gemini-2.0-flash-exp:free' # 3rd fallback
]
```

### GitHub Actions Workflow
Workflows should be modular. The daily job will have distinct steps for:
1.  Setup Python & Dependencies
2.  Run Ingestion (with time filtering and article limits)
3.  Run Scalable Processing (tiered analysis)
4.  Run Content Generation
5.  Build and Deploy Site
6.  Send Newsletter
