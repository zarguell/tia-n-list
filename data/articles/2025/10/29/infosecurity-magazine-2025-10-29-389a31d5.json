{
  "id": "infosecurity-magazine-2025-10-29-389a31d5",
  "source_id": "infosecurity-magazine",
  "guid": "https://www.infosecurity-magazine.com/news/open-source-b3-benchmark-security/",
  "title": "Open Source “b3” Benchmark to Boost LLM Security for Agents",
  "url": "https://www.infosecurity-magazine.com/news/open-source-b3-benchmark-security/",
  "published_at": "2025-10-29T10:45:00+00:00",
  "fetched_at": "2025-10-29T17:55:16.145424Z",
  "status": "processed",
  "content": {
    "raw": "The backbone breaker benchmark (b3) has been launched to enhance the security of LLMs within AI agents",
    "full": "The UK AI Security Institute (AISI) has partnered with the commercial security sector on a new open source framework designed to help large language model (LLM) developers improve security posture.\nThe backbone breaker benchmark (b3) is a new evaluation tool created by the AISI, Check Point and Check Point subsidiary Lakera. It’s designed to help developers and model providers improve the resilience of the “backbone” LLMs which power AI agents.\n“AI agents operate as a chain of stateless LLM calls – each step performing reasoning, producing output, or invoking tools,” Lakera explained in a blog post announcing the release.\n“Instead of evaluating these full agent workflows end-to-end, b3 zooms in on the individual steps where the backbone LLM actually fails: the specific moments when a prompt, file, or web input triggers a malicious output. These are the pressure points attackers exploit – not the agent architecture itself, but the vulnerable LLM calls within it.”\nTo help developers and model providers uncover these vulnerabilities before their adversaries do, b3 uses a new technique called “threat snapshots.” These micro tests are powered by crowdsourced adversarial data from Lakera’s “Gandalf: Agent Breaker” initiative.\nSpecifically, b3 combines 10 representative agent “threat snapshots” with a high-quality dataset of 19,433 Gandalf adversarial attacks. Developers can then use it to see how vulnerable their model is to attacks such as system prompt exfiltration, phishing link insertion, malicious code injection, denial-of-service and unauthorized tool calls.\nRead more on AI agent security: AI Chatbots Highly Vulnerable to Jailbreaks, UK Researchers Find\nThe b3 benchmark “makes LLM security measurable, reproducible, and comparable across models and application categories,” according to Lakera.\n“B3 lets us finally see which ‘backbones’ are most resilient in a given application, and what separates strong models from those that fail under pressure,” it said.\n“Along the way, the results revealed two striking patterns: models that reason step by step tend to be more secure, and open-weight models are closing the gap with closed systems faster than expected.”\nA Baseline For Improving LLM Security\nMateo Rojas-Carulla, co-founder and chief scientist at Lakera, argued that today’s AI agents are only as secure as the LLMs they’re powered by.\n“Threat Snapshots allow us to systematically surface vulnerabilities that have until now remained hidden in complex agent workflows,” he added.\n“By making this benchmark open to the world, we hope to equip developers and model providers with a realistic way to measure, and improve, their security posture.”\nAndrew Bolster, senior research & development manager (data science) at Black Duck, gave a cautious welcome to the new open source benchmark.\n“This type of research is a great baseline for agentic integrators to understand the threat model around these systems,” he argued.\n“But for true-scale security with AI in the mix, security leaders need to leverage both these novel prompt manipulation/benchmarking techniques, as well as battle-tested application security testing and model attestation regimes.”",
    "processed": ""
  },
  "analysis": {
    "score": 60,
    "relevance_score": 60,
    "threat_category": "None",
    "summary": "The UK AI Security Institute, in partnership with Check Point and Lakera, released the open‑source b3 benchmark to evaluate the security of backbone LLMs used in AI agents. The tool focuses on individual LLM calls that can trigger malicious outputs, using crowdsourced adversarial data from the Gandalf: Agent Breaker initiative.",
    "key_entities": [
      "UK AI Security Institute",
      "AISI",
      "Check Point",
      "Lakera",
      "b3",
      "Gandalf: Agent Breaker"
    ],
    "ttps": []
  },
  "content_source": "enhanced",
  "content_fetch_method": "trafilatura",
  "processing_metadata": {
    "processed_at": "2025-10-29T18:01:18.173299+00:00Z",
    "llm_provider": "unknown",
    "processing_method": "json_processing"
  },
  "updated_at": "2025-10-29T18:01:18.173850Z"
}