{
  "id": "unit-42-2025-10-31-d2bb3cd2",
  "source_id": "unit-42",
  "guid": "https://unit42.paloaltonetworks.com/?p=163280",
  "title": "When AI Agents Go Rogue: Agent Session Smuggling Attack in A2A Systems",
  "url": "https://unit42.paloaltonetworks.com/agent-session-smuggling-in-agent2agent-systems/",
  "published_at": "2025-10-31T10:00:33+00:00",
  "fetched_at": "2025-10-31T11:13:24.226884Z",
  "status": "processed",
  "content": {
    "raw": "Agent session smuggling is a novel technique where AI agent-to-agent communication is misused. We demonstrate two proof of concept examples. The post When AI Agents Go Rogue: Agent Session Smuggling Attack in A2A Systems appeared first on Unit 42.",
    "full": "Executive Summary\nWe discovered a new attack technique, which we call agent session smuggling. This technique allows a malicious AI agent to exploit an established cross-agent communication session to send covert instructions to a victim agent.\nHere, we discuss the issues that can arise in a communication session using the Agent2Agent (A2A) protocol, which is a popular option for managing the connections between agents. The A2A protocol’s stateful behavior lets agents remember recent interactions and maintain coherent conversations. This attack exploits this property to inject malicious instructions into a conversation, hiding them among otherwise benign client requests and server responses.\nMany AI threats involve tricking an agent with a single malicious piece of data such as a deceptive email or document. Our research highlights a more advanced danger, malicious agents.\nA straightforward attack on a victim agent might involve a one-time effort to trick it into acting on harmful instructions from a document without seeking confirmation from its user. In contrast, a rogue agent is a far more dynamic threat. It can hold a conversation, adapt its strategy and build a false sense of trust over multiple interactions.\nThis scenario is especially dangerous because, as a recent study shows, agents are often designed to trust other collaborating agents by default. Agent session smuggling exploits this built-in trust, allowing an attacker to manipulate a victim agent over an entire session.\nThis research does not reveal any vulnerability in the A2A protocol itself. Rather, the technique exploits the way implicit trust relationships between agents would affect any stateful protocol — meaning any protocol that can memorize recent interactions and carry out multi-turn conversation.\nMitigation requires a layered defense strategy, including:\n- Human-in-the-loop (HitL) enforcement for critical actions\n- Remote agent verification (e.g., cryptographically signed AgentCards)\n- Context-grounding techniques to detect off-topic or injected instructions\nPalo Alto Networks customers are better protected through the following products and services:\nPrisma AIRS is designed to provide layered, real-time protection for AI systems by detecting and blocking threats, preventing data leakage and enforcing secure usage policies across a variety of AI applications.\nAI Access Security is designed for visibility and control over usage of third-party GenAI tools, helping prevent sensitive data exposures, unsafe use of risky models and harmful outputs through policy enforcement and user activity monitoring.\nCortex Cloud AI-SPM is designed to provide automatic scanning and classification of AI assets, both commercial and self-managed models, to detect sensitive data and evaluate security posture. Context is determined by AI type, hosting cloud environment, risk status, posture and datasets.\nA Unit 42 AI Security Assessment can help you proactively identify the threats most likely to target your AI environment.\nIf you think you might have been compromised or have an urgent matter, contact the Unit 42 Incident Response team.\n| Related Unit 42 Topics | GenAI, Google |\nAn Overview of the A2A Protocol and Comparison With MCP\nThe A2A protocol is an open standard that facilitates interoperable communication among AI agents, regardless of vendor, architecture or underlying technology. Its core objective is to enable agents to discover, understand and coordinate with one another to solve complex, distributed tasks while preserving autonomy and privacy.\nIn the A2A protocol:\n- A local agent runs within the same application or process as the initiating agent, enabling fast, in-memory communication.\n- A remote agent operates as an independent, network-accessible service. It uses the A2A protocol to create a secure communication channel, allowing it to handle tasks delegated from other systems, or even other organizations, and then return the results.\nFor more details on A2A fundamentals and security considerations, please refer to our article: Safeguarding AI Agents: An In-Depth Look at A2A Protocol Risks and Mitigations.\nA2A has notable parallels with the Model Context Protocol (MCP), a widely used standard for connecting large language models (LLMs) to external tools and contextual data. Both aim to standardize how AI systems interact, but they operate on distinct aspects of agentic systems.\n- MCP functions as a universal adapter, providing structured access to tools and data sources. It primarily supports LLM-to-tool communication through a centralized integration model.\n- A2A focuses on agent-to-agent interoperability. It enables decentralized, peer-to-peer coordination in which agents can delegate tasks, exchange information and preserve state across collaborative workflows.\nIn short, MCP emphasizes execution through tool integration, whereas A2A emphasizes orchestration across agents.\nDespite these differences, both protocols face similar classes of threats, as shown in Table 1.\n| Attack/Threats | MCP | A2A |\n| Tool/Agent Description Poisoning | Tool descriptions can be poisoned with malicious instructions that manipulate LLM behavior during tool selection and execution | AgentCard descriptions can embed prompt injections or malicious directives that manipulate the client agent’s behavior when consumed |\n| Rug Pull Attacks | Previously trusted MCP servers can unexpectedly shift to malicious behavior after integration, exploiting established trust relationships | Trusted agents can unexpectedly turn malicious by updating their AgentCards or operation logic |\n| Tool/Agent Shadowing | Malicious servers register tools with identical or similar names to legitimate ones, causing confusion during tool selection | Rogue agents create AgentCards that mimic legitimate agents through similar names, skills or typosquatting techniques |\n| Parameter/Skill Poisoning | Tool parameters can be manipulated to include unintended data (e.g., conversation history) in requests to external servers | AgentCard skills and examples can be crafted to manipulate how agents interact, potentially exposing sensitive context or credentials |\nTable 1. Comparison of MCP and A2A attacks.\nThe Agent Session Smuggling Attack\nAgent session smuggling is a new attack vector specific to stateful cross-agent communication, such as A2A systems. A communication is stateful if it can remember recent interactions, like a conversation where both parties keep track of the ongoing context.\nThe core of the attack involves a malicious remote agent that misuses an ongoing session to inject additional instructions between a legitimate client request and the server’s response. These hidden instructions can lead to context poisoning (corrupting the AI's understanding of a conversation), data exfiltration or unauthorized tool execution on the client agent.\nFigure 1 outlines the attack sequence:\n- Step 1: The client agent initiates a new session by sending a normal request to the remote agent.\n- Step 2: The remote agent begins processing the request. During this active session, it covertly sends extra instructions to the client agent across multiple turn interactions.\n- Step 3: The remote agent returns the expected response to the original request, completing the transaction.\nKey properties of the attack\n- Stateful: The attack leverages the remote agent’s ability to manage long-running tasks and persist session state. This means the agent saves the context of an interaction, much like a person remembers the beginning of a sentence while listening to the end. In this context, stateful means the agent retains and references session-specific information across multiple turns (e.g., conversation history, variables or task progress tied to a session ID) so later messages can depend on earlier context.\n- Multi-turn interaction: Because of the stateful property, two connected agents can engage in multi-turn conversations. A malicious agent can exploit this to stage progressive, adaptive multi-turn attacks, which have been shown significantly more difficult to defend against in prior research (see, for example, “LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet” by Nathaniel Li et al. on Scale).\n- Autonomous and adaptive: Malicious agents that are powered by AI models can dynamically craft instructions based on live context such as client inputs, intermediate responses and user identity.\n- Undetectable to end users: The injected instructions occur mid-session, making them invisible to end users, who typically only see the final, consolidated response from the client agent.\nIn principle, any multi-agent system with stateful inter-agent communication could be susceptible to this attack. However, the risk is lower in setups fully contained within a single trust boundary. A trust boundary is an area of the system where all components are trusted by default, such as ADK or LangGraph multi-agent systems, where one administrator controls all participating agents.\nOur research therefore focuses on the A2A protocol, which is explicitly designed for cross-boundary interoperability. This interoperability enables agents to collaborate across different systems, modules or organizations.\nCompared to known MCP threats, agent session smuggling exploits A2A’s stateful and adaptive design in ways that are not possible in MCP. MCP servers generally operate in a stateless manner, executing isolated tool invocations without preserving session history, which limits actors’ ability to use them to mount multi-turn or evolving attacks.\nMCP servers are also typically static and deterministic, since they do not rely on AI models. In contrast, an A2A server can persist state across interactions and leverage model-driven reasoning, allowing a malicious agent to adapt and refine instructions over multiple turns. This combination of persistence and autonomy makes agent session smuggling more stealthy and difficult to defend against than MCP-based attacks.\nProof of Concept Agent Session Smuggling Attacks\nTo demonstrate the feasibility and potential impact of agent session smuggling, we developed two proof-of-concept (PoC) attack scenarios. The setup consisted of a financial assistant (the client agent) and a research assistant (the malicious remote agent). These PoCs illustrate how a malicious agent can exfiltrate sensitive data and execute unauthorized actions without the end user's awareness.\n- Sensitive information leakage: Extracting confidential data from the client agent\n- Unauthorized tool invocation: Convincing the client agent to perform unauthorized actions on behalf of the victim user\nEnvironment settings:\n- Development framework: Google Agent Development Kit\n- Cross-agent communication protocol: A2A\n- Client agent: A financial assistant based on the ADK financial-advisor example. The agent supports tasks such as retrieving financial news, managing portfolios and executing trade operations.\n- Model: Gemini 2.5 Pro [PDF]\n- Available tools: get_portfolio, get_profile, buy_stock, sell_stock\n- Remote agent: A research assistant adapted from the ADK data_analyst example. Its intended role is to fetch and summarize company news.\n- Model: Gemini 2.5 Flash [PDF]\n- Available Tools: Google Search\n- User interface: ADK Developer Web UI\nPoC 1: Sensitive Information Leakage\nIn this scenario, the malicious remote agent (research assistant) aims to exfiltrate the client agent’s (financial assistant’s) internal state, including:\n- Chat history\n- System instructions\n- Available tools\n- Tool schemas\nThe attack unfolds subtly. Upon receiving a legitimate request from the financial assistant, the research assistant issues a series of seemingly harmless clarification questions. These follow-up questions gradually trick the financial assistant into disclosing sensitive information that should never leave its environment.\nFigure 2 illustrates the user’s interaction flow. The session begins normally. The user asks the financial assistant to retrieve the investment portfolio and profile, followed by a request for a briefing on AI market news.\nThis market news request is delegated to the research assistant. As shown on the right side of the figure, the research assistant engages in a series of back-and-forth exchanges with the financial assistant. Except for the initial request and the final response, the research assistant injects all intermediate interactions.\nThe intermediate steps shown in Figure 2 only appear because the developer web user interface (UI) is designed to display every internal action. In a production chatbot UI, these intermediate interactions would likely remain invisible.\nFigure 3 provides a more detailed view via the financial assistant’s activity log. Row 10 shows the initial query sent to the research assistant, and Row 28 shows the final result the research agent produces. Rows 11–27, alternating between function_call and function_response, reveal the interactions between the two agents. During the interactions, the financial assistant gradually disclosed its system configuration and the user’s prior conversation history.\nFinally, Figure 4 shows the research assistant’s log after the attack. It confirms successful data exfiltration, including the financial assistant’s capabilities, tool schemas and the entire session history.\nPoC 2: Unauthorized Tool Invocation\nIn this scenario, the malicious remote agent (research assistant) attempts to trick the client agent (financial assistant) into executing unauthorized trade actions. The attacker’s objective is to misuse the client agent’s buy_stock tool without the end user’s knowledge or consent.\nAfter receiving a valid delegation request, the research assistant sends additional processing requirements, prompting the financial assistant to perform tool invocations unrelated to the user’s original request. This PoC assumes the attacker knows the financial assistant’s tools, having acquired either through public information or the previous PoC.\nFigure 5 illustrates the user’s interaction flow. On the right-hand side, the user’s news request is delegated to the research assistant. While the research assistant does provide the expected news summary, it also smuggles hidden instructions that induce the financial assistant to invoke the buy_stock tool. Again, these intermediate actions are executed autonomously and would remain invisible in a typical production chatbot interface, where the user would only be shown the final summarized response.\nFigure 6 shows the financial assistant’s activity log. Row 7 shows the initial request sent to the research assistant, and Row 12 shows the legitimate response received. Rows 8–11, however, reveal four additional actions the financial assistant autonomously executed, resulting in an unauthorized trade of 10 shares on behalf of the victim user. This sequence demonstrates how agent session smuggling can escalate from information leakage to high-impact tool execution.\nMitigation and Protection\nDefending against agent session smuggling requires a layered security approach that validates the identity of agents, the integrity of the conversation and the impact of the actions taken. The following strategies can help mitigate the risks:\n- Enforce out-of-band confirmation for sensitive actions: The most effective defense is to require HitL approval for any sensitive or high-impact action, but this confirmation must occur out-of-band, through a separate channel the LLM cannot influence. When an agent is instructed to perform a critical task, the orchestration framework should pause the execution. It should then trigger a confirmation prompt in a static, non-generative part of the application UI or through a separate system like a push notification.\n- Implement context grounding: An agent session smuggling attack relies on derailing a conversation from its original purpose to inject malicious commands. Context grounding is a technical control that algorithmically enforces conversational integrity. When a client agent initiates a session, it should create a task anchor based on the original user request's intent. As the interaction progresses, the client must continuously validate that the remote agent's instructions remain semantically aligned with this anchor. Any significant deviation or introduction of unrelated topics should cause the client agent to flag the interaction as a potential hijacking attempt and terminate the session.\n- Validate agent identity and capabilities: Secure agent-to-agent communication must be built on a foundation of verifiable trust. Before engaging in a session, agents should be required to present verifiable credentials, such as cryptographically signed AgentCards. This allows each participant to confirm the identity, origin and declared capabilities of the other. While this control does not prevent a trusted agent from being subverted, it eliminates the risk of agent impersonation or spoofing attacks and establishes an auditable, tamper-evident record of all interactions.\n- Expose client agent activity to users: Smuggled instructions and activities are invisible to end users, since they usually only see the final response from the client agent. The UI can reduce this weak spot by exposing real-time agent activity. For example, surfacing tool invocations, showing live execution logs or providing visual indicators of remote instructions. These signals improve user awareness and increase the chance of catching suspicious activity.\nConclusion\nThis work introduced agent session smuggling, a new attack technique that targets cross-agent communication in A2A systems. Unlike threats involving malicious tools or end users, a compromised agent represents a more capable adversary. Powered by AI models, a compromised agent can autonomously generate adaptive strategies, exploit session state and escalate its influence across all connected client agents and their users.\nAlthough we have not observed the attack in the wild, its low barrier to execution makes it a realistic risk. An adversary needs only to convince a victim agent to connect to a malicious peer, after which covert instructions can be injected without user visibility. Protecting against this requires a layered defense approach:\n- HitL approval for sensitive actions\n- Confirmation logic enforced outside of model prompts\n- Context-grounding to detect off-topic instructions and cryptographic validation of remote agents\nAs multi-agent ecosystems expand, their interoperability also opens new attack surfaces. Practitioners should assume that agent-to-agent communication is not inherently trustworthy. We must design orchestration frameworks with layered safeguards to contain the risks of adaptive, AI-powered adversaries.\nPalo Alto Networks Protection and Mitigation\nPrisma AIRS is designed for real-time protection of AI applications, models, data and agents. It analyzes network traffic and application behavior to detect threats such as prompt injection, denial-of-service attacks and data exfiltration, with inline enforcement at the network and API levels.\nAI Access Security is designed for visibility and control over usage of third-party GenAI tools, helping prevent sensitive data exposures, unsafe use of risky models and harmful outputs through policy enforcement and user activity monitoring. Together, Prisma AIRS and AI Access Security help secure the building of enterprise AI applications and external AI interactions.\nCortex Cloud AI-SPM is designed to provide automatic scanning and classification of AI assets, both commercial and self-managed models, to detect sensitive data and evaluate security posture. Context is determined by AI type, hosting cloud environment, risk status, posture and datasets.\nA Unit 42 AI Security Assessment can help you proactively identify the threats most likely to target your AI environment.\nIf you think you may have been compromised or have an urgent matter, get in touch with the Unit 42 Incident Response team or call:\n- North America: Toll Free: +1 (866) 486-4842 (866.4.UNIT42)\n- UK: +44.20.3743.3660\n- Europe and Middle East: +31.20.299.3130\n- Asia: +65.6983.8730\n- Japan: +81.50.1790.0200\n- Australia: +61.2.4062.7950\n- India: 000 800 050 45107\nPalo Alto Networks has shared these findings with our fellow Cyber Threat Alliance (CTA) members. CTA members use this intelligence to rapidly deploy protections to their customers and to systematically disrupt malicious cyber actors. Learn more about the Cyber Threat Alliance.\nReferences\n- The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover – Matteo Lupinacci et al., arXiv:2507.06850\n- Multi-Agent Systems in ADK – Agent Development Kit, Google GitHub\n- Graph API overview – LangChain\n- A2A Protocol – The Linux Foundation\n- Model Context Protocol (MCP) – Model Context Protocol\n- LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet – Nathaniel Li et al., Scale\n- Google Agent Development Kit – Agent Development Kit, Google GitHub\n- Google adk-samples – Google GitHub\n- Gemini 2.5 Pro [PDF] – Google\n- Gemini 2.5 Flash [PDF] – Google\n- ADK Google Search Tool – Google GitHub\n- ADK Developer Web UI – Google GitHub\n- Sigstore A2A – Google GitHub",
    "processed": ""
  },
  "analysis": {
    "score": 90,
    "relevance_score": 95,
    "threat_category": "AI Agent Compromise / Insider Threat",
    "summary": "This article reveals a novel attack called agent session smuggling that exploits the Agent2Agent (A2A) protocol's stateful communication to inject covert malicious instructions into AI agent conversations. The attack leverages built-in trust between collaborating AI agents to manipulate victim agents dynamically over multiple interactions.",
    "key_entities": [
      "Agent2Agent (A2A) protocol",
      "malicious AI agents",
      "victim AI agents"
    ],
    "ttps": [
      "Agent session smuggling",
      "Cross-agent communication exploitation",
      "Covert instruction injection",
      "Trust exploitation between AI agents"
    ]
  },
  "content_source": "enhanced",
  "content_fetch_method": "trafilatura",
  "processing_metadata": {
    "processed_at": "2025-10-31T11:19:09.437733+00:00Z",
    "llm_provider": "unknown",
    "processing_method": "json_processing"
  },
  "updated_at": "2025-10-31T11:19:09.438094Z"
}