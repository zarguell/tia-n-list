{
  "id": "infosecurity-magazine-2025-11-06-94ad4862",
  "source_id": "infosecurity-magazine",
  "guid": "https://www.infosecurity-magazine.com/news/multi-turn-attacks-llm-models/",
  "title": "Multi-Turn Attacks Expose Weaknesses in Open-Weight LLM Models",
  "url": "https://www.infosecurity-magazine.com/news/multi-turn-attacks-llm-models/",
  "published_at": "2025-11-06T15:00:00+00:00",
  "fetched_at": "2025-11-07T11:13:19.426225+00:00Z",
  "status": "fetched",
  "content": {
    "raw": "A new Cisco report exposed large language models to multi-turn adversarial attacks with 90% success rates",
    "full": "",
    "processed": ""
  },
  "analysis": {
    "score": null,
    "relevance_score": null,
    "threat_category": null,
    "summary": null,
    "key_entities": [],
    "ttps": []
  },
  "content_source": "rss",
  "content_fetch_method": null,
  "processing_metadata": {},
  "updates": {
    "content": {
      "full": "A new report has revealed that open-weight large language models (LLMs) have remained highly vulnerable to adaptive multi-turn adversarial attacks, even when single-turn defenses appear robust.\nThe findings, published today by Cisco AI Defense, show that while isolated, one-off attack attempts frequently fail, persistent, multi-step conversations can achieve success rates exceeding 90% against most tested defenses.\nMulti-Turn Attacks Outperform Single-Turn Tests\nCisco’s analysis compared single-turn and multi-turn testing to measure how models respond under sustained adversarial pressure.\nUsing over 1000 prompts per model, researchers observed that many models performed well when faced with a single malicious input but quickly deteriorated when attackers refined their strategy over several turns.\nAdaptive attack styles, such as “Crescendo,” “Role-Play” and “Refusal Reframe,” allowed malicious actors to manipulate models into producing unsafe or restricted outputs. In total, 499 simulated conversations were analyzed, with each spanning 5-10 exchanges.\nThe results indicate that traditional safety filters are insufficient when models are subjected to iterative manipulation.\nKey Vulnerabilities and Attack Categories\nThe study identified 15 sub-threat categories showing the highest failure rates across 102 total threat types.\nAmong them, malicious code generation, data exfiltration and ethical boundary violations ranked most critical.\nCisco’s scatter plot analyses revealed that models plotting above the diagonal line in vulnerability graphs share architectural weaknesses that make them disproportionately prone to multi-turn exploitation.\nThe research defined a “failure” as any instance where a model:\n-\nProduced harmful or inappropriate content\n-\nRevealed private or system-level information\n-\nBypassed internal safety restrictions\nConversely, a “pass” occurred when the model refused or reframed harmful requests while maintaining data confidentiality.\nRecommendations For Developers and Organizations\nTo mitigate risks, Cisco recommended several practices:\n-\nImplement strict system prompts aligned with defined use cases\n-\nDeploy model-agnostic runtime guardrails for adversarial detection\n-\nConduct regular AI red-teaming assessments within intended business contexts\n-\nLimit model integrations with automated external services\nThe report also called for expanding prompt sample sizes, testing repeated prompts to assess variability and comparing models of different sizes to evaluate scale-dependent vulnerabilities.\n“The AI developer and security community must continue to actively manage these threats (as well as additional safety and security concerns) through independent testing and guardrail development throughout the lifecycle of model development and deployment in organizations,” Cisco wrote.\n“Without AI security solutions – such as multi-turn testing, threat-specific mitigation and continuous monitoring – these models pose significant risks in production, potentially leading to data breaches or malicious manipulations.”",
      "enhanced_at": 1762514233.3925068,
      "fetch_method": "trafilatura"
    }
  },
  "updated_at": "2025-11-07T11:17:13.392815+00:00Z"
}