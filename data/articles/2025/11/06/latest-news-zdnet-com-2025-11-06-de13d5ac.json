{
  "id": "latest-news-zdnet-com-2025-11-06-de13d5ac",
  "source_id": "latest-news-zdnet-com",
  "guid": "0e28fd10-d211-4361-8285-cadd58988f3a",
  "title": "Microsoft researchers tried to manipulate AI agents - and only one resisted all attempts",
  "url": "https://www.zdnet.com/article/microsoft-researchers-tried-to-manipulate-ai-agents-and-only-one-resisted-all-attempts/",
  "published_at": "2025-11-06T20:01:25+00:00",
  "fetched_at": "2025-11-07T11:16:09.626426+00:00Z",
  "status": "fetched",
  "content": {
    "raw": "AI agents can buy and sell on our behalf. Here's why that should worry us.",
    "full": "",
    "processed": ""
  },
  "analysis": {
    "score": null,
    "relevance_score": null,
    "threat_category": null,
    "summary": null,
    "key_entities": [],
    "ttps": []
  },
  "content_source": "rss",
  "content_fetch_method": null,
  "processing_metadata": {},
  "updates": {
    "content": {
      "full": "Microsoft researchers tried to manipulate AI agents - and only one resisted all attempts\nFollow ZDNET: Add us as a preferred source on Google.\nZDNET's key takeaways\n- Microsoft studied interactions between AI customers and vendors.\n- Most agents failed to resist manipulation and make wise choices.\n- The results underscore the dangers of an AI agent-run economy.\nAs you've probably noticed, there's been a lot of hype circulating around AI agents and their supposed potential to transform the economy and human labor by automating routine, time-consuming tasks. A growing body of research, however, shows that agents fall short in elementary ways, indicating that they're probably not ready for primetime just yet.\nAlso: I let Gemini Deep Research dig through my Gmail and Drive - here's what it uncovered\nNew research from Microsoft found that industry-leading agentic AI tools struggle to interact with one another to complete basic marketplace decisions, like choosing a restaurant by comparing menu offerings and prices. Researchers also found most agents fell for manipulation attempts, including prompt injections and misleading information. These agents failed consistently, though, meaning the research could provide a blueprint for AI companies to address those flaws moving forward.\nA test marketplace\nMicrosoft's research revolved around what it calls the \"Magentic Marketplace\" -- an open-source environment where AI agents converse with one another in order to complete transactions in a virtual environment simulating a real-world marketplace. (You can give it a try yourself on GitHub.)\nThe goal was to test the practical capabilities of agentic systems at a time when AI developers are rapidly delivering more autonomous products, like shopping and buying agents for both individuals and businesses. OpenAI's Operator, for example, can navigate websites and complete purchases on behalf of users, while Meta's Business AI can interact with customers like an automated sales representative.\nAlso: Google Finance gets a Gemini-powered upgrade - what it can do for you now\nThe rise of automated buyers and vendors \"hint at a future where [AI] agents become active market participants, but the structure of these markets remains uncertain,\" Microsoft wrote in a company blog post about its new research.\nThe Magentic Marketplace is an early attempt to map out some of that structure, and to reveal any traps that we might be heading into. Designed to simulate the complexity of real-world markets, it involves numerous agents, all of which are set loose, in true game theory style, to interact in an effort to optimize their own, individual outcomes -- rather than just pitting an automated customer agent against a buyer agent and letting them strike a deal.\nMicrosoft ran its experiments using leading proprietary models like GPT-5 and Gemini 2.5 Flash, as well as open-source models like OpenAI's OSS-20b. Those models were used to simulate 100 customers and 300 businesses, which could interact with one another via text prompts that can be monitored by human users.\nAlso: I let ChatGPT Atlas do my Walmart shopping for me - here's how the AI browser agent did\nMicrosoft assigned customer agents a list of items and amenities and had to search through all available vendor agents to find the one that offered everything they were looking for at the best available price. The researchers used a \"consumer welfare\" metric to assess the performance of each model, which was calculated as the sum of a customer's internal item valuations minus the final sales price, aggregated across all of its transactions.\nHow agents fared\nAccording to Microsoft, the customer agents often showed promise in helping humans overcome what the company described as \"information gaps.\" Think of these as mental or logistical shortcuts a human might take when presented with too many options, like choosing randomly or searching for the cheapest option.\n\"This change matters because as agents gain better tools for discovery and communication, they relieve customers of the heavy cognitive load of filling any information gaps,\" Microsoft wrote in its blog post. \"This lowers the cost of making informed decisions and improves customer outcomes.\"\nAlso: Why Amazon really doesn't want Perplexity's AI browser shopping for you\nThe agents also showed some critical flaws, though.\nOne of the big problems had to do with what the researchers call the \"Paradox of Choice\" -- a more familiar phrase might be \"analysis paralysis.\" Basically, even though they had many different options to choose from, most of the customer agents -- with the exception of GPT-5 and Gemini 2.5 Flash -- only interacted with a small number of vendor agents.\n\"This suggests that most models do not conduct exhaustive comparisons and instead easily accept the initial 'good enough' options,\" Microsoft wrote. The researchers additionally found that for every customer agent, consumer welfare decreased as the number of available options for vendor agents increased.\nAlso: Google's AI mode agents can snag event tickets for you now â€“ here's how\nThe researchers also tested six different \"manipulation strategies\" to try to mislead the customer agents, including adding dubious claims like \"#1-rated Mexican restaurant\" or using overt prompt injections. There was a wide degree of variation in terms of how the models responded, according to Microsoft; notably, Claude Sonnet 4 showed total resistance to all attempts at manipulation.\nUnsurprisingly, the researchers detected a few biases that hindered model performance. For example, open-source models like Qwen2.5-14b-2507 tended to choose the last business that was offered in the initial list of options, regardless of how it compared to the others. There was also a widespread \"proposal bias,\" which caused models to choose the first vendor agent that engaged with it with an offer, suggesting a prioritization of speed over thoroughness.\n\"These biases can create unfair market dynamics, drive unintended behaviors, and push businesses to complete on response speed rather than product or service quality,\" Microsoft said.\nThe risks of using agents\nWhile the companies behind these tools promote them as time-saving personal assistants, they could also have major economic implications -- the likes of which have yet to be mapped out. The stock market, for example, is already governed by inscrutable algorithms designed to track the prices of innumerable goods. How much more opaque will that system become when AI isn't just tracking the prices of commodities, but actually overseeing many or the majority of everyday transactions?\nAlso: AI agents are only as good as the data they're given, and that's a big issue for businesses\nSince we already know that AI models are subject to all kinds of biases that hide deep in the intricacies of their training data, how will those manifest themselves when legions of AI consumers and buyers are unleashed into the wild?\nMicrosoft's findings are just the latest to prove that agents shouldn't be trusted in high-stakes situations, and whenever they are deployed, they should be carefully monitored.\nAnother study published earlier this week, for example, found that AI agents are a long way away from completing quality freelance work. An Anthropic research project earlier this year showed that Claude struggled to operate a small business for a month.\nWant more stories about AI? Sign up for our AI Leaderboard newsletter.\nAll of these results point to the conclusion that despite the huge amount of hype swirling around agents, it'll be a while before these systems are able to function autonomously. As Microsoft concludes in its blog post: \"Agents should assist, not replace, human decision-making.\"",
      "enhanced_at": 1762514201.0208213,
      "fetch_method": "trafilatura"
    }
  },
  "updated_at": "2025-11-07T11:16:41.021189+00:00Z"
}