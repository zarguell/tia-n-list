{
  "id": "the-hacker-news-feeds-feedburner-com-2025-11-19-3092731c",
  "source_id": "the-hacker-news-feeds-feedburner-com",
  "guid": "https://thehackernews.com/2025/11/servicenow-ai-agents-can-be-tricked.html",
  "title": "ServiceNow AI Agents Can Be Tricked Into Acting Against Each Other via Second-Order Prompts",
  "url": "https://thehackernews.com/2025/11/servicenow-ai-agents-can-be-tricked.html",
  "published_at": "2025-11-19T09:59:00+00:00",
  "fetched_at": "2025-11-19T11:18:04.403525+00:00Z",
  "status": "fetched",
  "content": {
    "raw": "Malicious actors can exploit default configurations in ServiceNow's Now Assist generative artificial intelligence (AI) platform and leverage its agentic capabilities to conduct prompt injection attacks. The second-order prompt injection, according to AppOmni, makes use of Now Assist's agent-to-agent discovery to execute unauthorized actions, enabling attackers to copy and exfiltrate sensitive",
    "full": "",
    "processed": ""
  },
  "analysis": {
    "score": null,
    "relevance_score": null,
    "threat_category": null,
    "summary": null,
    "key_entities": [],
    "ttps": []
  },
  "content_source": "rss",
  "content_fetch_method": null,
  "processing_metadata": {}
}