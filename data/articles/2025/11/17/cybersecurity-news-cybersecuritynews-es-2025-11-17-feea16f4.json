{
  "id": "cybersecurity-news-cybersecuritynews-es-2025-11-17-feea16f4",
  "source_id": "cybersecurity-news-cybersecuritynews-es",
  "guid": "https://cybersecuritynews.es/?p=42336",
  "title": "Cómo certificar que los modelos de IA son seguros y fiables antes de su lanzamiento al mercado",
  "url": "https://cybersecuritynews.es/como-certificar-que-los-modelos-de-ia-son-seguros-y-fiables-antes-de-su-lanzamiento-al-mercado/",
  "published_at": "2025-11-17T22:00:00+00:00",
  "fetched_at": "2025-11-18T11:20:18.284794+00:00Z",
  "status": "fetched",
  "content": {
    "raw": "La inteligencia artificial (IA) generativa se ha convertido en una herramienta esencial para la competitividad empresarial y cada vez son más las compañías que invierten en este tipo de tecnologías con el objetivo de acelerar su crecimiento y rentabilidad. Se estima que la IA generativa podría sumar entre 2,6 y 4,4 billones de dólares de productividad anual en todo el mundo, según un informe de McKinsey. Sin embargo, transformar este potencial en resultados tangibles no siempre es sencillo. De hecho, un número elevado de iniciativas de desarrollo de modelos de IA se quedan estancadas en la “fase piloto”. Según la encuesta de S&amp;P Global Market Intelligence 2025, el 42% de las empresas ha abandonado este año la mayoría de sus iniciativas de IA, respecto al 17% de proyectos que no se terminaron en 2024. Además, un 46% de los proyectos de IA no pasan de la fase piloto, según datos de Gartner. En este contexto,Galtea, empresa especializada en desarrollar tecnología para validar sistemas de IA generativa, subraya la importancia de certificar la seguridad y fiabilidad de los sistemas antes de su lanzamiento. Y, en este sentido, destaca algunas de las formas de certificar que los modelos de IA son seguros y fiables antes de lanzarlos al mercado: “El primer paso para garantizar el éxito es reducir el margen existente entre la realidad, las expectativas y los resultados de las empresas”, afirma el CEO de Galtea, Jorge Palomar, que señala también que “es clave preguntarse antes de empezar qué pasos tienen que dar para&nbsp; asegurar que sus sistemas de IAson seguros y precisos, contando con herramientas que les permitan validarlos, y de esta forma asegurar su fiabilidad, seguridad y cumplimiento regulatorio”: Pruebas de seguridad y fiabilidad Un paso fundamental es contar con pruebas de seguridad y fiabilidad en la IA. Cada vez más empresas integran sistemas de IA en procesos críticos, desde la atención al cliente hasta la detección de fraudes o la generación de contenido. Sin embargo, pocas cuentan con procesos robustos de validación que garanticen su comportamiento seguro y predecible en entornos reales. Datos sintéticos y usuarios simulados Las empresas tienen que someter sus sistemas de IA a pruebas exhaustivas, a través de datos de alta calidad y específicos del caso de uso. Para poder hacer esto escalable, es clave la generación de datos sintéticos y usuarios simulados (en ocasiones, con intervención y supervisión humana) que permiten probar los sistemas de IA en situaciones realistas. Si una empresa quiere automatizar su call center con IA generativa, necesita examinar de forma exhaustiva cómo se va a comportar el sistema en producción, y asegurar mediante la generación de evidencias que lo hará de forma fiable, segura y robusta. “Se trata de reproducir un escenario real de interacciones de potenciales usuarios reales a escala, que ofrezca información rigurosa del comportamiento del sistema, y permita una buena gestión de calidad y riesgos, además de entender el potencial del caso de uso para escalar”, explica el CEO de Galtea. Detectar errores y probar la solución IA a escala El triunfo pasa por detectar aquellos errores que generan los sistemas de IA y, con ello, evitar que se produzcan cuando el sistema llegue a producción. Otro punto importante es probar la solución IA a escala y prepararla para que su rendimiento sea eficiente en el mundo real. Por todo ello, lograr el éxito pasa por contar con herramientas y proveedores especializados que les acompañen en el proceso de configuración y validación de dichos sistemas antes de salir al mercado. En palabras de Jorge Palomar, “la validación debe ser un requisito previo, no un paso posterior al despliegue. Por ello y, sin duda, lograr el éxito pasa por contar con herramientas y proveedores especializados que acompañen a las empresas en este proceso antes de salir al mercado. De hecho, en aquellos casos en los que las empresas confían en un proveedor especializado, la probabilidad de éxito del proyecto es del 67%, según un informe del MIT. En paralelo, cabe señalar que las empresas que utilizan la tecnología de Galtea han logrado una aceleración del 80% en los procesos de validación y un incremento del 25% en el rendimiento de sus modelos”. La entrada Cómo certificar que los modelos de IA son seguros y fiables antes de su lanzamiento al mercado se publicó primero en CyberSecurity News.",
    "full": "",
    "processed": ""
  },
  "analysis": {
    "score": null,
    "relevance_score": null,
    "threat_category": null,
    "summary": null,
    "key_entities": [],
    "ttps": []
  },
  "content_source": "rss",
  "content_fetch_method": null,
  "processing_metadata": {}
}