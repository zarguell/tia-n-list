{
  "id": "gbhackers-security-1-globally-trusted-cyber-security-news-platform-gbhackers-com-2025-11-04-e7365be6",
  "source_id": "gbhackers-security-1-globally-trusted-cyber-security-news-platform-gbhackers-com",
  "guid": "https://gbhackers.com/?p=167947",
  "title": "Hackers Can Manipulate Claude AI APIs with Indirect Prompts to Steal User Data",
  "url": "https://gbhackers.com/hackers-can-manipulate-claude-ai-apis/",
  "published_at": "2025-11-04T05:49:14+00:00",
  "fetched_at": "2025-11-04T12:50:10.554079+00:00Z",
  "status": "fetched",
  "content": {
    "raw": "A new security issue discovered by researchers reveals that Anthropic’s Claude AI system can be exploited through indirect prompts, allowing attackers to exfiltrate user data via its built‑in File API. The attack, documented in a detailed technical post on October 28, 2025, demonstrates how Claude’s Code Interpreter and API features could be manipulated to send [&#8230;] The post Hackers Can Manipulate Claude AI APIs with Indirect Prompts to Steal User Data appeared first on GBHackers Security | #1 Globally Trusted Cyber Security News Platform.",
    "full": "",
    "processed": ""
  },
  "analysis": {
    "score": null,
    "relevance_score": null,
    "threat_category": null,
    "summary": null,
    "key_entities": [],
    "ttps": []
  },
  "content_source": "rss",
  "content_fetch_method": null,
  "processing_metadata": {}
}