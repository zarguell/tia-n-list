{
  "id": "schneier-on-security-2025-11-11-ea7d1693",
  "source_id": "schneier-on-security",
  "guid": "https://www.schneier.com/?p=71125",
  "title": "Prompt Injection in AI Browsers",
  "url": "https://www.schneier.com/blog/archives/2025/11/prompt-injection-in-ai-browsers.html",
  "published_at": "2025-11-11T12:08:48+00:00",
  "fetched_at": "2025-11-12T11:17:31.972165+00:00Z",
  "status": "fetched",
  "content": {
    "raw": "This is why AIs are not ready to be personal assistants: A new attack called &#8216;CometJacking&#8217; exploits URL parameters to pass to Perplexity&#8217;s Comet AI browser hidden instructions that allow access to sensitive data from connected services, like email and calendar. In a realistic scenario, no credentials or user interaction are required and a threat actor can leverage the attack by simply exposing a maliciously crafted URL to targeted users. [&#8230;] CometJacking is a prompt-injection attack where the query string processed by the Comet AI browser contains malicious instructions added using the &#8216;collection&#8217; parameter of the URL. LayerX researchers say that the prompt tells the agent to consult its memory and connected services instead of searching the web. As the AI tool is connected to various services, an attacker leveraging the CometJacking method could exfiltrate available data. In their tests, the connected services and accessible data include Google Calendar invites and Gmail messages and the malicious prompt included instructions to encode the sensitive data in base64 and then exfiltrate them to an external endpoint. According to the researchers, Comet followed the instructions and delivered the information to an external system controlled by the attacker, evading Perplexity&#8217;s checks. I wrote previously: Prompt injection isn&#8217;t just a minor security problem we need to deal with. It&#8217;s a fundamental property of current LLM technology. The systems have no ability to separate trusted commands from untrusted data, and there are an infinite number of prompt injection attacks with no way to block them as a class. We need some new fundamental science of LLMs before we can solve this.",
    "full": "",
    "processed": ""
  },
  "analysis": {
    "score": null,
    "relevance_score": null,
    "threat_category": null,
    "summary": null,
    "key_entities": [],
    "ttps": []
  },
  "content_source": "rss",
  "content_fetch_method": null,
  "processing_metadata": {}
}