{
  "id": "bleeping-computer-2025-11-05-3336565e",
  "source_id": "bleeping-computer",
  "guid": "https://www.bleepingcomputer.com/news/security/google-warns-of-new-ai-powered-malware-families-deployed-in-the-wild/",
  "title": "Google warns of new AI-powered malware families deployed in the wild",
  "url": "https://www.bleepingcomputer.com/news/security/google-warns-of-new-ai-powered-malware-families-deployed-in-the-wild/",
  "published_at": "2025-11-05T14:59:59+00:00",
  "fetched_at": "2025-11-06T11:12:59.689736+00:00Z",
  "status": "fetched",
  "content": {
    "raw": "Google's Threat Intelligence Group (GTIG) has identified a major shift this year, with adversaries leveraging artificial intelligence to deploy new malware families that integrate large language models (LLMs) during execution. [...]",
    "full": "",
    "processed": ""
  },
  "analysis": {
    "score": null,
    "relevance_score": null,
    "threat_category": null,
    "summary": null,
    "key_entities": [],
    "ttps": []
  },
  "content_source": "rss",
  "content_fetch_method": null,
  "processing_metadata": {},
  "updates": {
    "content": {
      "full": "Google's Threat Intelligence Group (GTIG) has identified a major shift this year, with adversaries leveraging artificial intelligence to deploy new malware families that integrate large language models (LLMs) during execution.\nThis new approach enables dynamic altering mid-execution, which reaches new levels of operational versatility that are virtually impossible to achieve with traditional malware.\nGoogle calls the technique \"just-in-time\" self-modification and highlights the experimental PromptFlux malware dropper and the PromptSteal (a.k.a. LameHug) data miner deployed in Ukraine, as examples for dynamic script generation, code obfuscation, and creation of on-demand functions.\nPromptFlux is an experimental VBScript dropper that leverages Google's LLM Gemini in its latest version to generate obfuscated VBScript variants.\nIt attempts persistence via Startup folder entries, and spreads laterally on removable drives and mapped network shares.\n\"The most novel component of PROMPTFLUX is its 'Thinking Robot' module, designed to periodically query Gemini to obtain new code for evading antivirus software,\" explains Google.\nThe prompt is very specific and machine-parsable, according to the researchers, who see indications that the malware's creators aim to create an ever-evolving \"metamorphic script.\"\nGoogle could not attribute PromptFlux to a specific threat actor, but noted that the tactics, techniques, and procedures indicate that it is being used by a financially motivated group.\nAlthough PromptFlux was in an early development stage, not capable to inflict any real damage to targets, Google took action to disable its access to the Gemini API and delete all assets associated with it.\nAnother AI-powered malware Google discovered this year, which is used in operations, is FruitShell, a PowerShell reverse shell that establishes remote command-and-control (C2) access and executes arbitrary commands on compromised hosts.\nThe malware is publicly available, and the researchers say that it includes hard-coded prompts intended to bypass LLM-powered security analysis.\nGoogle also highlights QuietVault, a JavaScript credential stealer that targets GitHub/NPM tokens, exfiltrating captured credentials on dynamically created public GitHub repositories.\nQuietVault leverages on-host AI CLI tools and prompts to search for additional secrets and exfiltrate them too.\nOn the same list of AI-enabled malware is also PromptLock, an experimental ransomware that relies on Lua scripts to steal and encrypt data on Windows, macOS, and Linux machines.\nCases of Gemini abuse\nApart from AI-powered malware, Google's report also documents multiple cases where threat actors abused Gemini across the entire attack lifecycle.\nA China-nexus actor posed as a capture-the-flag (CTF) participant to bypass Gemini's safety filters and obtain exploit details, using the model to find vulnerabilities, craft phishing lures, and build exfiltration tools.\nIranian hackers MuddyCoast (UNC3313) pretended to be a student to use Gemini for malware development and debugging, accidentally exposing C2 domains and keys.\nIranian group APT42 abused Gemini for phishing and data analysis, creating lures, translating content, and developing a \"Data Processing Agent\" that converted natural language into SQL for personal-data mining.\nChina's APT41 leveraged Gemini for code assistance, enhancing its OSSTUN C2 framework and utilizing obfuscation libraries to increase malware sophistication.\nFinally, the North Korean threat group Masan (UNC1069) utilized Gemini for crypto theft, multilingual phishing, and creating deepfake lures, while Pukchong (UNC4899) employed it for developing code targeting edge devices and browsers.\nIn all cases Google identified, it disabled the associated accounts and reinforced model safeguards based on the observed tactics, to make their bypassing for abuse harder.\nAI-powered cybercrime tools on underground forums\nGoogle researchers discovered that on underground marketplaces, both English and Russian-speaking, the interest in malicious AI-based tools and services is growing, as they lower the technical bar for deploying more complex attacks.\n\"Many underground forum advertisements mirrored language comparable to traditional marketing of legitimate AI models, citing the need to improve the efficiency of workflows and effort while simultaneously offering guidance for prospective customers interested in their offerings,\" Google says in a report published today.\nThe offers range from utilities that generate deepfakes and images to malware development, phishing, research and reconnaissance, and vulnerability exploitation.\nAs the cybercrime market for AI-powered tools is getting more mature, the trend indicates a replacement of the conventional tools used in malicious operations.\nThe Google Threat Intelligence Group (GTIG) has identified multiple actors advertising multifunctional tools that can cover the stages of an attack.\nThe push to AI-based services seems to be aggressive, as many developers promote the new features in the free version of their offers, which often include API and Discord access for higher prices.\nGoogle underlines that the approach to AI from any developer \"must be both bold and responsible\" and AI systems should be designed with \"strong safety guardrails\" to prevent abuse, discourage, and disrupt any misuse and adversary operations.\nThe company says that it investigates any signs of abuse of its services and products, which include activities linked to government-backed threat actors. Apart from collaboration with law enforcement when appropriate, the company is also using the experience from fighting adversaries \"to improve safety and security for our AI models.\"\n7 Security Best Practices for MCP\nAs MCP (Model Context Protocol) becomes the standard for connecting LLMs to tools and data, security teams are moving fast to keep these new services safe.\nThis free cheat sheet outlines 7 best practices you can start using today.",
      "enhanced_at": 1762427884.0509882,
      "fetch_method": "trafilatura"
    }
  },
  "updated_at": "2025-11-06T11:18:04.051349+00:00Z"
}